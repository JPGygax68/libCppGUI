2016-07-15
==========

(Continuing on "oriented geometry")

It becomes clear now that the original Rectangle structure should be replaced throughout the whole library by a templated version that takes axis direction into account (at least the Y direction, but I see little harm in supporting the same parameter to the X axis as well).

And so, we get the following possible combinations (disregarding the flipped X axis option):

   | Y axis              | Rectangle orientation | Starting point | Ending point   | 
-- | ------              | --------------------- | -------------- | ------------   |
 1 | top-down (standard) | left-to-right         | x              | x + length     |
 2 |                     | right-to-left         | x + length     | x              |
 3 |                     | top-down              | y              | y + length     |
 4 |                     | bottom-up             | y + length     | y              |
 5 | bottom-up           | left-to-right         | x              | x + length     |
 6 |                     | right-to-left         | x + length     | x              |
 7 |                     | top-down              | y + length     | y              |
 8 |                     | bottom-up             | y              | y + length     |

 
What we see here is that all that is really needed is two types of segments: segments that follow the direction of the axis (1, 3, 5, 8), and segments that go against the axis (2, 4, 6, 7). Thus, the parameters on the Oriented_segment<> data type (which may not be needed as such) would simply be "along the axis" vs "against the axis".

However, orientation as defined thus far is not just along/against the axis, but also which axis we're working with. In order to obtain a truly adaptive Oriented_rectangle<> template, we need to parameterize it with the "direction" [better term?] of both axes:

template<bool BackwardX, bool BackwardY>
struct Rectangle { ... };

[to be continued...]

2016-07-13
==========

It is necessary at this point get clarity on how to handle geometry in libCppGUI.

So far, the coordinate system has been assumed to be: X axis pointing to the right, Y axis pointing downward, with the origin being located in the top-left corner. While there is no immediate need to change that, we need to know what this is going to look like in the future; too much time has already been lost trying to make things work without sufficient planning.

Several different things need to be considered when handling geometry:

1) The coordinate system. As pointed out above, only one is being supported right now, but it should be made configurable in the future. (Most probably, the direction of the Y axis - and maybe of the X axis as well - will be reported by Renderer implementations as constexpr data; thus, data types that need to know about that can be parameterized with the standard Config parameter.)

2) Cultural orientations. No such orientations have been defined yet (other than as experimental values), and when they are, they will most probably be defined as simple aliases for geographical orientations. It should be noted that these aliases will likely be more numerous that just "<culture>-major" and "<culture>-minor"; it is possible that other cultures use different orientations for different purposes.

3) Layouting orientation. This is the problem I've been trying to address with Oriented_point<>, Oriented_extents<>, and Oriented_rectangle<>, but layouting orientation probably cannot be handled completely independently of the other two factors.
  
Layouting orientation determines the order in which layout elements are placed inside a container. It can be used to generalize a layout algorithm in such a way that it works for all four cardinal orientations (top-down, left-to-right, bottom-up, right-to-left).

What is needed, therefore, are standardized methods/functions that can be used to divide up the rectangular space available in a container (the "content rectangle") without having to know which way that container is oriented, or what coordinate system is being used.

The current implementations of the Oriented...<> classes are well on their way to fulfilling the former requirement, but not the latter.

(continuing the next night...)

Working with an axis that happens to be "pointing the wrong way" becomes possible when 
  a) axis orientation 
  b) intended orientation
  c) starting position and 
  d) length 
are all handled together. In C++, this can be achieved elegantly using the template-assisted strong typing system.

It is only while writing this that I now realize that defining a type Oriented_rectangle<> was perhaps not a very useful step, since the layouting code I wrote thus far does not handle more than one dimension. (But actually, even a "grid" type layout, which does work on both dimensions, probably won't be needing such a rectangle type - though it might still turn out to be useful.)

The concept that really helps here is that of "oriented segments", typed with the orientation (left-to-right, top-down, etc.) and the "axis sign" (for lack of a better term; what I mean is whether the coordinate in- or decreases along the intended orientation).

Thus, a segment defined by a starting point of 5 and a length of 10 can actually define a whole a lot of different segments: vertical and downward, vertical and upward, horizontal and left-to-right, horizontal and right-to-left.

Most importantly, each direction can be the result of going either along or against the corresponding axis: if the Y axis grows downward and the intended direction is top-down, the segment would start 5 units from the top and end at position 15. But with the Y axis oriented the same way and bottom-up as the intended direction, the segment would again start at y = 5, but go *up* and thus end at -5!




2016-07-11
==========

It is possible I may not be able to do significant work on libCppGUI in the near future, so I'd like to consolidate a few thoughts on the next steps here before switching to another task.

I now have code that allows using run time- and build time-configured Box Model implementations interchangeably. (I an unfinished state: the layouting functionality still needs to be broken out and made into an aspect).

It occurred to me that a similar principle could and should be applied to container layouting. The case is however a little different: whereas the Box Model concept only needs a single run-time and a single build-time implementation (for now), container layouting needs several implementations (possibily an ever-growing number) that all should be available at both build and run time.

Is is possible to abstract away the difference between build-time and run-time implementations ? I was thinking along the lines accessing aspects via a pointer that could be either `this` or something else [requiring a friend declaration]. However this would probably not be simple and straighforward - more thinking is required.

.. thinking done (hopefully). Here are the main points:

- All access to the Container Layouter aspect of a class is newly done via a `container_layouter()` accessor method.

- The Container Layouter implementation is injected into the class template as a "typename" template parameter "Layouter" that is a wrapper struct for a normalized aspect (avoids the over-verbose "template" template parameters, along with other benefits)

- One of these implementations is called `Container_layouter_delegator` (maybe some shorter form can be found); its implementation of `container_layouter()` returns a pointer. The `container_layouter()` methods of all other implementations simply return `this`.

- With the `Container_layouter_delegator`, the pointer returned by `container_layouter()` returns an instance of a Container Layouter aspect that has been made "autonomous": instead of accessing the main class via a statically cast `this` pointer, it actually stores a real pointer to the main class and uses that instead. [This difference can be injected via a base class template with two specializations; that base class belongs into `util.hpp` or a similar support module.]

- "Autonomized" Container Layouters must manually inherit from the Container_base Layouter aspect (since they do not otherwise ultimately inherit from Container_base<>).

- Container<> is the template where the Container layouter is injected - Container_base<> only propagates the layouting methods to children.


2016-07-09
==========

23:35
-----

I completed the first phase of replacing the box model concept, and the test app compiles once again and produces decent visuals. Some work however remains to be done:

1) The current implementations of the box model concept do not yet separate out the layouting functionality into separate (sub-) aspects.

2) For the runtime variant, there is currently no way to define default values.

    [I have begun working on this, but didn't finish yet. One thing that needs to be done is to apply the CRTP pattern to `Container_base` (and probably to all `..base` classes), so that `Box_model` get access to overloaded default values defined as constexpr members of the implementation class. 
    One variant of this would be to turn `container_base` into a standardized aspect. ]

2016-07-08
==========

21:44
-----

The first step towards making libCppGUI capable of working with a future designer app consists in simplifying the box model along the lines I've outlied earlier today.

This means:

- A box model template that is parameterized with, on top of the standard parameters Config and With_layout, a boolean that indicates whether the box model will be "dynamic" or "static" (those parameters go to the outer struct, i.e. the aspect "wrapper"); and the aspect template parameters according to the updated standard, which uses a CRTP `Class` parameter on top of the `Parent` parameter used for aspect chaining.

- Two specializations, one for dynamic, one for static configuration.
  - These two specializations could (should?) inherit from a common that provides common functionality, such as rendering.

- A layouter aspect that can be injected into the box model template. That one does not need the dynamic/static parameter, as it will simply work with whatever the box model main aspect provides.

Also, the `Box` intermediary class, which used to provide box-related functionality based on the data provided by the `Box_model` implementations, should no longer be required, as by definition, the new `Box_model` will work with standardized box model properties.


--------

A new challenge is appearing on the horizon, in the form of an expressed need (desire ?) by a customer to be able to design user interfaces manually - as opposed to programmatically, as I was hoping I could convince him would be good enough for the time being.

This desire is "inconvenient", because a) writing a designer application would be a lot of work under the best of conditions, and b) the whole architecture does not feel very structurally stable at the moment, increasing the risk of having to rewrite big parts of such an application in the not-so-distant future.

One very specific problem mandates careful assessment right now: the fact that I have tried to make as many parameters as possible "build-time" by defining them as parameters of the class templates. This approach, of course, is almost diametrically opposed to any attempts at run-time configuration.

The problem stands out most prominently with the box model. I had planned on creating many templated implementations of that concept, practically all of which taking template parameters for border width and color, margins, and inner padding. I have already created a few implementations that were using the same values for all four cardinal directions, and was planning on adding implementations that could have direction-specific values. I had also vaguely planned to create implementations that could be parameterized at run-time.

However, I see no practical way to make a designer create template instances with varying parameters at run time, short of invoking a build chain to generate code on-the-fly and then linking it dynamically. While such an approach would be interesting and might have applications more interesting still, I do not have it at my disposal right now.

[Thinking about this further, coming up with a minimal such system might not be quite as difficult as it sounds. CMake could be used as a convience to handle platform specifics. However, this leaves open the question of how to (dynamically) link such code with the designer application. Because the designer app cannot instantiate the templates, it would need to access widgets via an abstraction layer.
All in all, this idea should not yet be discarded entirely, as it may yield long-term benefits.]

The only alternative I can see right now is to take a step backwards and to move customization back into the runtime domain. In order not to completely loose the ability to optimize at build time, I would need to come up with a reusable pattern that allows parameters to be interchangeably build or run time.

This would mean that for each customizable template, a boolean template parameter should be added that determines whether or not customization will be build or run time.

When run time customization is specified, template specialization will inject the necessary properties for the values to be set programmatically.

I'm not too sure what to do if compile-time customization is chosen. So far, I've implemented all customization via template parameters, but I'm not sure that is the best or even the correct way of doing things.

Thinking aloud here... what about doing the same thing as for run time customization, but with constexpr getters only (no setters) ? This would require the code to be universal, as optimizations via specialization would no longer be available.

How would user code that customizes a widget (or aspect thereof) actually look like ? CRTP could be used to access the getters, so that injection could be written as simple getters in the derived class (with defaults provided in the class/aspect itself).

The question remains how a widget customized at run-time should be "solidified" into a compile-time customized version when the UI code is being generated - and whether it is worthwhile supporting this at all.

It would involve generating a derived class for each combination of parameters that are being used in a given UI. This certainly seems possible. It would involve using the properties metadata - which is needed anyway for the benefit of the designer app - to generate a getter for each property.

Another question is how exactly to implement properties. I think the key here is to make use of the already existing "aspect" mechanism, which ensures that all parts of a class (or at least, all parts that can have properties) share a single line of inheritance (multiple inheritance is restricted to injecting property-less facilities).

`Widget<>` or `Abstract_widget<>` defines a static constexpr method `metadata()`, which returns a data structure of type `Metadata`. `Metadata` would be composable, making it possible for any derived class or aspect to override the inherited implementation of `metadata()` with its own, which calls the inherited one and adds its own metadata to the result.

Such metadata should include: the name of the class or aspect, and a list of property descriptors. Each property descriptor should include: the name of the property; its type (string, boolean, float, or enumeration).

One question remains open: how to work with widgets or aspects that can be customized with data types, one example being `Slider<>`, which could benefit from taking the type of its value as a parameter, which would allow it to support integer, floating point, and even rational numbers with the same code base. I think the only practical way to support this is to hardcode all sensible choices into the designer application, which implies that any widget class has at most one such template parameter.


2016-06-30
==========

- There is a problem with focus cycling


2016-06-11
==========

A plan is needed to (already) overhaul the new Layout Managers. The problem is that they are now dependent on a specific type of layouting data (padding + spacing), which a) may not be enough in all cases, and b) is sometimes not needed at all.

### Idea 1

One possibility would be to handle the possible extra layouting parameters by overloading the layouting methods, e.g.: 

- `layout(Container_base &)`
- `layout(Container_base &, const Padding &)`
- `layout(Container_base &, const Padding &, Width spacing)`
- `layout(Container_base &, Width spacing)`
  
[and similar for get_minimal_size()]

This would put the burden on the Layouter aspects of specific Container_base descendants to invoke the layout manager with the right parameters.

### Idea 2

Another possibility would be to again overload the layout manager methods, but this time with specialized Container descendants:

- `layout(Container_base &)` (no padding, border, or spacing)
- `layout(Container<Config, true, Bordered_box> &)` (border only)
- `layout(Container<Config, true, Bordered_padded_box> &)` (border and padding)
- `layout(Container<Config, true, Bordered_padded_box_with_margins> &)` (margins, border, padding)

The advantage is that the Layouter aspects no longer have to adapt by themselves. 

Another possible advantage is that fallbacks are possible: a layout manager that does not explicitly support specialized containers would still support the "naked" Container_base. Though that is probably not worth a whole lot in practice, as all layout managers would have to be support all varieties eventually.

### Idea 3

Yet another idea is to define the Box concept in such a way that the layouting algorithms can stay identical whatever box model is being used. This could be achieved by defining methods on the Box concept implementations that abstract away the differences between those implementations, such as:

- `wrapping_width(border) -> Width`: returns the distance between outer and inner rectangle for the given direction (N, E, S, W), OR 
- `inner_rectangle(const Rectangle &) -> Rectangle`, achieving the same goal in a different way ?

Those methods would typically be inline and static and/or constexpr and thus wouldn't incur any real runtime cost.
As an added bonus, layout managers could still provided specialized implementations should the need arise.

----

Clearly, idea (3) is the most promising.

Something else is now bothering me about this, it's that layouting managers as currently defined do work that could be used else where, specifically to do the internal layouting of buttons and other types of "boxed" elements. Thinking about it though, there are more differences than similarities, and the work is not complicated anyway. The best way to think about this is probably to add some conviences routines to the Box concept anyway, most efficiently by defining a common CRTP base class, e.g. `Box_model`.

Correction to the above. In fact, with the current concept, the box model will have to be injected at different places in the class inheritance graph: somewhere below Widget<> for non-container widgets, and somewhere below Container_base<> for container widgets.

This means that either

  a) `Widget<>` methods making use of the box model (e.g. `draw_borders()`) need to be given a reference to the specific box instance they are supposed to work with, or
  b) Box model-related methods be injected below `Widget<>`

The combination of (a) and (b) would be to derive helper classes from Box_model specializations that know about `Widget<>` and can be easily injected at any level.

It would also be conceivable to inject the box model as a component (HAS_A) instead of an ancestor (IS_A), which however leads to the usual problem that empty structs cannot be optimized away from the memory layout.

In fact, that problem is the reason why this project's "pseudo-aspects" were developed, and indeed the `Box` functionality should be treated as an aspect.

So, we need:

- a `Box_model` concept
- a `Box` aspect, parameterized with the `Box_model` implementation (though it may take on additional parameters later on)


2016-06-10/11 (after midnight)
=============

I tried to simplify the Container hierarchy today and removed the Container<> class, rewiring things so as to keep the compatibility (as far as possible). That introduced a few bugs, which still need to be resolved.

Part of what I've done was to have Root_widget derive from Container instead of from Abstract_container. This may have been a mistake, as Widget includes the ability the be a child of an Abstract_container, which Root_widget does not need. I will probably have to undo that change.

Because of the above mistake, I had considered moving the invalidation locking functionality to the Window class. It may still make sense to do that.

Another thought is that the Window concept implementation should provide compile-time information about the way updating should work - though that may also depend on the graphics adapter, so the exact mechanics are still subject to more thinking and planning.


2016-06-08
==========

- Is it legal, or should it be made legal, to update a widget (e.g. call Textbox<>::change_text() ) before that widget has been "initialized" (method init(), better term and method name still pending) ?

 
2016-06-08
==========

- Stringlist needs to visually reflect the fact that it has focus.
- Stringlist does not take focus when clicked on items (scrollbar does take focus though)

- I really need a better alternative to assert()

- TODO: move the layout managers into a subdirectory, with a file pair for each


2016-06-07
==========

- Stringlist (and other containers with specific layouting) should very probably inherit from Container_base<> instead of Container<>.
  -> [now done for Stringlist<>]


2016-06-06
==========

- TODO: check: when container triggers mouse_enter() on child, is mouse_motion() still being relayed to said child ?


2016-06-02
==========

- Should items be selected upon mouse button down or mouse button up ?
  -> clearly: down!

2016-06-01
==========

- When scrolling the Stringlist (or a Scrollbox) and the mouse pointer was resting on an item, highlighting it, that item stays "hovered" even though it no longer is. I "fixed" this for the Stringlist by manually resetting the hovered item index to -1, but this is clumsy, and the Scrollbox derivates still have the problem.
  To really address the problem, any "scrolling" operation (the corresponding methods are still not defined yet, still using standard invalidation as placeholders) should also trigger a re-evaluation of the mouse position.
    -> however, in the case of the Stringlist (NOT the Scrollbox family), it would be sufficient to just apply the item delta to the index of the hovered item.
    
- The UI seems sluggish. I don't know why that should be the case; the time may be ripe now for some profiling. Maybe Resharper can help with that ?


2016-05-26
==========

A few TODOs emerged from today's cleanup work (aimed at creating a reusable skeleton project - without success so far).

- The Icon_resources module (templated struct) gives access to binary (pre-rasterized font) data. Looking at the compiler warnings today, it turned out that the fact that the accessor methods are constexpr makes them automatically inline (of course - in hindsight). 
  The data returned by the accessors is obtained via a header file, called "all_icon_fonts.h", which is generated at build time. Unfortunately, `all_icon_fonts.hpp` doesn't just declare that data, it actually inserts it into the source via `#include` directives.
  This is wasteful in terms of build times. What needs to be done here is split that file into a declaration and a source part, which latter should of course take the form of a `.ipp` file since font and glyph sizes cannot be known before building the library consumer.
  
- Regarding the same module, it turns out that grouping all "icon resources" into a single "provider" templated struct may be suboptimal. The reason for that is that different "icon" glyphs are consumed at different pixel sizes, which will lead to wasted const data since the current `CMakefile.txt` code rasterizes all of the requested glyphs for each requested pixel size.
  It would certainly help to upgrade the CMake code that rasterizes and converts to hex the icon glyphs. But when doing so, the opportunity should also be used to upgrade the `gpcbin2c` utility: instead of just generating comma-separated hex codes and leave it to the programmer to use those to generate constant data, the utility should be able to produce self-contained header and source files on its own, giving the user a choice between standard C arrays and C++ `std::array`s for more flexibility.
  Also, the functionality of `gpcbin2c` should be made available as a C++ class, so that `gpcfontrasterizer` can be upgraded to generate source code on its own, which would greatly simplify the CMake setup.


2016-05-24
==========

Some important changes today.

- I've reverted back to using implementation files. The compilation times were becoming unacceptable, and I had misunderstood the way explicit template instantiation works - it has no effect on inline methods, and all methods implemented inside a class declaration are automatically inline.

- I've moved the user event injection methods from Abstract_widget<> to Widget<>. Since only Widget instances can be put into containers, there is no sense injecting at a higher level (the implementations were empty anyway).

- I've also removed mouse_click() from the injection interface, and added a "clicks" parameters (of type Count) to mouse_button(). mouse_click() still exists, but it's now generated by Widget<>::mouse_button(), no longer by GUI_window.


2016-05-23
==========

Today's work has revealed a couple of fundamental design questions, to which I have provided answers that need to be considered provisional.

The most important question is whether or not, or more probably exactly how and to what extent, user code may change the user interface from within event handlers.

The provisional answer I gave to that is: "yes, you may"; but I've already had to fix the mouse_click() handler of Button<> to make this work, and more correctional work may be needed.

The fundamental problem burns down to this: if event handlers are allowed to modify the user interface, then methods triggering events must allow for the possibility that their object instance ceases to exist while the event handler executes, and it would therefore be illegal to access any methods or fields after the event handler returns. Even worse, if multiple event handlers are going to be supported in the future, the dispatch mechanism would have to provide a way to stop subsequent handlers either from being called or from accessing removed widgets.

The problem could be further compounded by the possibility of implementations to have the UI be rendered in a different thread - something that, so far, is not accounted for in any way, but could be necessary in the case of, say, a game engine that uses a separate rendering thread.

It would probably be best to define a locking method that guarantees that the UI can be modified safely.

- When the platform renders the UI inside the same thread that also responds to input events, no synchronization is necessary.

- Even when no locking is necessary, debug builds should still guard against as many potential bugs as possible (such as user code asking for a UI access lock more than once, or libCppGUI code trying to access widget members after an event handler has had UI access)

- When the UI is rendered in a separate thread, access must be protected by a real mutex.
  
- Alternatively, UI-modifying code could be deferred. This would involve using callbacks (usually lambdas). For such a solution to be efficient, compilers would have to be able to optimize lambda expressions that are called immediately - which according to an experiment I just conducted on http://gcc.godbolt.org/, they are not. Perhaps a solution could be devised with the help of a couple of macros.

2016-05-11
==========

There are open questions concerning the relationship between layout() and init().

The fundamental problem is that layout() is designed to be called before init(), yet can also be called afterwards (such as after a resize of the containing window).

So far, I've ignored the implications - but they do exist. For example, the thumb of an already operating scrollbar will need to be updated after a reflow, both because the scrollbar itself has changed in length, but also because it is very likely that the scrollable it is connect to has changed too.

What is needed here is an additional entry point in the Layouter aspect hierarchy, to be called automatically from init() but also after a reflow. The definition of this entry point is something like "compute view from data" (e.g. for the scrollbar, the data is the position that it represents/controls).

compute_view_from_data() should be a virtual function defined in Abstract_widget<>. Most existing widgets should be updated accordingly.

Addendum:

I had some confusion about the right time / place to call Root_widget<>::init(). This was owed to the fact that init(), before now, had two distinct responsibilities: to connect the GUI elements with the backends, and to compute the views. Because layouting was kept separate, I had so far missed that latter part because it appeared to be a small one (which is actually not true, because the Textbox at least needs to do quite a bit of stuff there).

Having cleared that up, it becomes clear now that init() must be implemented in such a way as to be completely independent of the layouting and other view-related parameters, so that it can be called *before* run-time layout().

Actually, it might be a good idea to rename init() to something more explicit, such as connect_backends().

2016-05-09
==========

I let myself get bogged down by over-thinking this stylesheet business. I'll try to recap before letting the matter rest for now.

- The management of font handles is probably implemented in the wrong place right now, i.e. in the root widget, when it acually belongs with the Canvas (which might delegate it further to a "resource holder" if the graphics resources should be shared with other Canvas instances)

- At this time, colors are described statically, so there is no need for member variables to hold their untranslated values; and since translation is trivial and build-time (constexpr), there is no need to store the translated values either.
  - This will change when stylesheets are introduced. It will make sense then to store the translated value at "style application" time (no need to cache the untranslated value).
  
So, even though at the current time there are resources (i.e. colors) that require neither the untranslated nor the translated value to be store, this situation is temporary, and there is little point trying to use the "members-via-inheritance" trick to work around the waste of space that would otherwise be introduced by the "no zero-sized data members" rule of C++.

What is required though is a specializable template that is capable of holding either untranslated, translated or both handles of a given resource, with hooks for init() and cleanup().

I'm going to spend the next couple of hours implementing this.


2016-05-06
==========

My thinking process regarding stylesheets does not seem to be making real progress. I'll now try to come up with a pragmatic approach that will enable me to continue without getting bogged down by a big re-design. What do I need ?

- Styling parameters (colors, border withs, font parameters) must be defined outside of the code itself (there are many such stopgaps in the current code)

- Some of these parameters must be adapted for use by the subsystems (graphics, sound later on)
  - This happens during init()
    - init() is therefore the opportunity to "kill two birds with one stone": 1) obtain the style definitions (which could be a somewhat involved process once stylesheets finally come into play) 2) obtain the adapted resources (primarily fonts)
      - It also means that style changes would require a cleanup() (yet-to-be introduced opposite of init()) followed by a new execution of init(). This actually looks like a rather elegant compromise (between full CSS-like freedom and lightweight implementation)

- Inheriting style definitions from containers will have to happen, though not necessarily now.

- The same is true of run-time assignable stylesheets - though actually, this is a feature that would usually not be needed at all and that I therefore relegate to low priority.

So, the way forward for now looks like this:

- All style definitions are moved to inheritable methods.
  - These methods can be static or constexpr for now.
  - Later on, these methods will redirect to stylesheets [actually, the support for stylesheets could be encapsulated into aspects, of which there could then be variants with and without stylesheet support]

---

Addendum: self-aware structs (http://duriansoftware.com/joe/Self-aware-struct-like-types-in-C++11.html, though the "self-aware" part may not be needed) might be of use to access resources without caring about whether or not they need adapting (works around the C++ limitation of not supporting 0-sized structs):
  - for each resource:
    - define a struct that has entry points for init() and cleanup() plus an accessor get()
      - init() and cleanup() can be no-ops if the resource does not need translating
      - get() will either be a pass-through or return the handle of the translated resource
    - have the widget class inherit from that struct OR put it into a grouping struct
    
If resource structs are grouped, meta-programming (apply()) can be used for initialization and cleanup


2016-05-05
==========

About styles, again
-------------------

- Is it really necessary/desirable that they can be changed at run-time ?
  - Possible "middle ground": do not allow switching at run-time, but re-generate the UI (possibly using persistency to minimize the impact on the user) ?
  
- Is it desirable to make stylesheets static ?
  - PROs: less run-time costs
  - CONs: types are no longer identical -> unnecessary polymorphism (with an extra inheritance level) would be required to "hide" differences
  
=> Design decision: stylesheets are consulted at run-time (not specifying when exactly yet)

- How / when are stylesheets applied ?
  - Because they can influence layouting, stylesheets are needed when layouting (which may be at design-time or at run-time)
    - This may also mean that some style definitions are not needed in layout-less instantiations, such as padding
    
- Should styles really cascade ? Does it make sense that e.g. the button border width may be defined anywhere in a hierarchy ?
  - While this makes sense in the browser, I'm not convinced it serves a real need in a standard GUI.
    - Example 1: A property panel embedded in a larger UI, where properties are displayed with a smaller font to save space.
      - the font size need only be known at layout time
      - the canvas font handle (NOT the rasterized font) should be inherited from the panel
        -> individual widgets should probably cache the font handle so they don't have to climb the hierarchy to get it
  
- The content of stylesheets should be computed statically, so that the compiler and linker can throw away unneeded stuff.
  - But in order to avoid polymorphism either via virtual methods (run-time bloat) or template parameters on the consumers (which would introduce type incompatibilities between otherwise identical components, see above), the structure of the stylesheets needs to be normalized
    -> Templatized factories producing normalized stylesheet objects (via a constexpr factory method) ?
    
- How to make stylesheets extendable, e.g. to support third-party widgets ?
  - Relatively easily: by deriving. The problem however is when multiple third-party extensions are involved that do not know each other.
    - Each widget defines a struct with the run-time styles it needs
    - 
    
... trailing off ...
  
2016-05-03
==========

A thought about borders: I think I should change the way I used to think about them. Borders are NOT a universal concept to be applied everywhere - I guess that sort of thinking came chiefly from my experience with CSS.

Instead, borders should be used where needed, and only there: around text input fields ("textbox"), list boxes, "combo" boxes, etc. (in fact the presence of the word "box" is a clear indicator of where borders are of use).


2016-05-01
==========

I have made good, if slow, progress on the Listbox these past days. A few things are worth noting:

- take_focus() now propagates upwards, in the sense that if a widget refuses to take the focus, it will still inform its container, so that the container, in turn, may take the focus upon itself (which it should do anyway to put itself in the focus chain).

- There is a need to have visual feedback clearly indicating which widget has the focus. In particular, it is annoying that, even though a listbox can now be (indirectly) focus by clicking on its scrollbar (or any of its children, though with a list of buttons, that cannot be done without clicking one of the buttons), there is no indication that the listbox is now ready to be navigated with the cursor keys.

### Other things to think about

- Exception safety:
  - When is it ok for a widget to throw an exception ? 
  - Can it be ensured that the GUI will stay in a stable state after an exception was thrown ?
  - Callbacks should be upgraded to real events
    - Is there a motivation to use aspects to support both simple callbacks or full-blown signal/slot handling ?
  - What about concurrency ?
    - Must all operations on the GUI be serialized, or is concurrency allowable (and useful) in certain cases ?
      - e.g. a worker thread reporting progress via a progress bar ?
        - typically, the worker thread might change the value of the progress bar, then call invalidate() which would be responsible for queueing a redraw by the main thread
      - loading an image (e.g. for a supposed image viewer widget) and preparing that image for use by the graphics subsystem could be done in a worker thread IF the graphics wrapper supports this (e.g. with OpenGL, a secondary context could be created). If however it does not, then the graphics wrapper should provide a means to bring that operation into the main thread transparently (doable via a lambda, which the compiler can (hopefully) optimise into inline code when deferring is unnecessary)

- States in debug builds ?
  - It may be useful at some point to protect library users (or developers, just as much) from avoidable mistakes by enforcing (in debug builds only) discrete states, with all methods being associated with one or more of those states and only valid for calling when the widget (or entire GUI) is in one of those states. E.g.:
    - defining the user interface (through set_...() methods) may only be done when in "setup" state
    - the "layouting" state is only available if the library is configured to carry that aspect
      - However layouting can still occur for specific widgets if the aspect has been specifically enabled on them. In such a case, however, "layouting" would be a secondary state and concurrent to the GUI's "main" state.
    - init() will put the GUI into "initializing" state (and only be allowed following the "setup" or the "layout" state)
    - event handlers require the "ready" state
      - ? is there a way to prevent loops in event handling ?
        -> probably yes, but not with a state machine - it might involve a lot of "friend" statements instead
    - rendering requires the "drawable" state (which can only be entered via the platform adapter) (actually, the exact interplay between the platform adapter and the library proper will need some thinking about)
    - some states could be thread-local, while most would probably be global (to a given root widget, or even to the whole process)
  
  - Another thought: all adapters (graphics, keyboard, mouse, sound, etc.) should define a method them of state changes. Of course those methods are only allowed to carry out checks - they musn't do any work required in release builds because they won't be called then.
  
  - The state machine would most certainly "live" in the root widget.
  
2016-04-28
==========

First implementation attempt of the Listbox failed (as in "almost worked").

Working on it revealed a problem I had forgotten about: the fact that the thumb cannot usually be perfectly synchronized with the content area it is linked with.

It is therefore important, when moving the thumb by dragging it, that the thumb *movement* be used as input, and NOT its position!

It may also be helpful to "normalize" the thumb's position after a dragging operation has ended - in fact, that would probably be the only way to ensure that the thumb can reliably be moved to either end position.

It may also mean that implementing the Listbox as a specialization of Scrollbox was a bad idea. Though the current implementation of the Scrollbox only has a vertical scrollbar, that will change in the future, while a Listbox usually does not have a horizontal scrollbox; in fact, it would be possible to compute the minimal width of a Listbox on the basis of the widest of its contained items, all but removing the remaining similarities.
  
  -> Q: is there a legitimate use case for a list box with a horizontal scrollbar ? -> YES, thinking about it, this is quite possible
  
Therefore, is it still possible and a good idea to implement Listbox as a derivation of Scrollbox ?

I think the answer is YES: all it really takes is a way to customize the navigation, i.e. to leave it to the concrete implementation to decide by how much to travel when one of the buttons is pushed, the mousewheel is used, the slide range is clicked either before or after the thumb, or the equivalent action is performed via the keyboard.

Touch-based navigation, implementation of which hasn't started yet, is another incentive to have a single implementation.

Instead of derivation, how about using composition ? Let the scrollbox ask its content pane how far to travel, while to scrollbox still does the travelling itself ?

  -> Might work, but what about selecting/focusing/highlighting items ?
    
    -> This must be done using a "bring into view" mechanism, which is quite distinct from scrolling itself:
      - Scrollbox gets input (scrollbar, keyboard) for "up/down one item"
      - Scrollbox sends that input along to its content pane
      - Content pane changes its "selected item", notes its position
      - Content pane tells container scrollbox to "bring into view" rectangle of newly selected item
  
About travelling:

  - With a standard scrollbox, the travelled amount is arbitrary for "single step", while the amount for "page up" / "page down" is simply the visible size (= the inner rect of the scrollbox), possibly minus a configurable "overlap".
  
  - With a listbox, the "single step" is the step from the beginning of the first visible item to the beginning of the next one.
    
Possible approach:

- Scrollable_pane as a specialization of Container, adding the ability to communicate with the scroll box:
  - notify the scrollbox of changes to its size (scrollbox then updates the offset of the pane, and the position of the thumb)

- Grid_pane (or, for the time being, List_pane), as a specialization of Scrollable_pane, adding the abilities:
  - Informing the container (the scrollbox) about the travelling distance for "up" and "down" (separately), which may depend on the height of the first/last item
  
The problem with this is that it requires either unnecessary virtual methods, or two separate specializations of Scrollbox.

An acceptable compromise would be to combine delegated navigation with default navigation, by giving precedence to the former.

To avoid requiring forking Scrollbox, delegated navigation could be introduced in Scrollable_pane as inline no-ops (even constexpr); the concrete type of the content pane could then be injected into Scrollbox as a template parameter.

This appears (and is) somewhat complicated, but does offer the advantage of greater flexibility, and thus less hassle when implementing things like grids and even treeviews later on.

---------

The Scrollbar class is becoming a problem, as one other class to redesign. I believe it is a good strategy to redefine its responsibility now: it should no longer try to keep track of a position by itself (as it does now, in the form of a fraction), but leave that to its client, which will in turn update the position and size of the thumb.

--------------

Idea: implement notification (such as "Position_change" in Scrollbar) as nil-able aspects ?


2016-04-27
==========

There is a need to define an interface between the Scrollbox component and its content.

The current state is working, but treats the content pane as a simple, uniform surface, which can be moved by arbitrary amounts of pixels. This does not take into account the size of items contained in the pane.

What is to be done?

- Implement a completely separate component "Listbox" ? 
  - Or is that name reserved for a component that contains items that are not themselves widgets (indeed typically simple text strings) ?
  
"Listbox":

- A scrollbox combined with a list of widgets

- Those widgets will be arranged into a vertical "stack", the width of each widget set to the "inner width" of the listbox, and its height to the minimum of the highest item (which will get stored to the main aspect and used to configure the scrollbox as well). This will be done as part of the layouting process.
  -> This implies that adding/removing widgets will only be possible if layouting is enabled at least for the Scrollbox component.
  
- Regardless of possible differences in height, each "item widget" counts as a single step navigation-wise

- Layouting of the scrollbox will be based on a configurable number (on the layouter aspect) of "maximum visible items"
  -> NO, that is not possible -> it might be done for get_minimal_size() though

- It is probably best to put the items into a sub-container with "stack" layouting (which may need to be slightly adapted)

2016-04-07
==========

- After implementing Glyph_button, I found that there is currently no way to enforce an aspect ratio, which would make glyph buttons look much nicer.
  -> Future extension ? More sophisticated layouting ?
  
2016-04-05
==========

Working on a text input dialog. Not intended to be reusable yet, except by copy-pasting it into Locsim Instructor.

Several things need work:

- Using MaterialIcons is problematic because apparently the boxing isn't quite correct
  -> THIS APPEARS TO BE WRONG. I've just downloaded and installed a tool called Type Light, and it shows that the MaterialDesign icons are perfectly centered
  -> However, glyphs appear to be systematically shifted by one pixel to the left and the top - but this is actually wrong: the top and left padding is one pixel, where the bottom and right is two pixels - meaning it's impossible at that size to get perfect centering because of grid-fitting.

2016-03-27
==========

- Silly problem: when scrolling with the wheel and the mouse cursor passes onto another child widget without being moved, the highlight does not change until the user actually moves the mouse

2016-03-23
==========

Right now, setting the value of a widget (e.g. "text" on a Textbox) requires a a check (or possibly more later on) to find out whether or not the rasterized font is available.

What is missing at this time is an awareness about whether or not a widget is "live"; or, if maintaining that state turns out to be avoidable, a distinction between *initializing* values versus *updating* them (possibly via overloaded versions of the property setters ?).

I do think, however, that a one-time initialization method is required, to be called after de-serializing or programmatically definining the UI.
If layouting is enabled, the corresponding layout() on the root widget would need to occur immediately before that.

From the above follows a rule:

THE LAYOUT OF A WIDGET *SHOULD NOT* DEPEND ON ITS VALUE (OR SET OF VALUES).

The reason is that this would make it harder to predict the visual appearance of a UI right when it first appears, which is already quite a bit uncertain as soon as font sizes etc. are no longer fixed.

==> TODO: implement:

  1) a virtual method initialize()
  2) property setters that do not trigger invalidation or layout recalculation
      - try to find names that express the difference, rather than employing overloads or prefixed versions of existing names
  
2016-03-14
==========

How to implement trigger_redraw() in a way that will get optimal run-time behaviour on any possible combination of backends ? Brainstorming.

- For a game-type application where frames are redrawn at a high rate independently of GUI interactions, trigger_redraw() could be a no-op
  - Caveat: this does not apply, however, if the GUI is rendered to a dedicated off-screen buffer.
  
- When the only reason to redraw the GUI is in response to user interaction, the best approach is to collect and delay redraw operations until the input event that triggered the need to redraw something has been fully handled.

  - Caveat: the fact that an input event has been fully handled does not necessarily mean that the (partial or full) redraw can or should be done right away. It is conceivable (though probably not very common) that it could happen in another thread. This would make sense if, for example, the GUI is rather complex and rendering it in the same thread as the "event pump" could reduce responsiveness.
  
- For old-style hardware with only 2D acceleration, or when rendering directly to video memory without the help of a GPU (e.g. Linux framebuffer), it can make sense to execute redraws immediately.

- Under the same circumstances, certain operations such as scrolling can be optimized so that only some parts of the affected area need to be redrawn.

- Modern hardware can also help reduce the need to redraw, though in different ways (off-screen buffers / images in video memory ("textures")).

- In rare cases of very low-performance hardware, it could make sense to make redraws interruptible: if filling in an area uncovered by scrolling takes too long, and new input events are already pending, the user experience may be better served by "pushing back" the redraws.

---------------

The above requirements are pretty diverse. It seems important to me that a programmer need not concern himself with all these details, not even when adapting the library for new backends. It is therefore imperative not only to provide abstractions for drawing operations, but also to come up with an impeccable architecture to support said abstractions.

### "Scrolling"

I used the term "scrolling" above, but that is actually a user-space term. "Blitting" was the technical term used for fast transfers of image portions to and from a framebuffer, which is what made scrolling (at reasonable speeds) possible back then. And though such technology is unlikely to come back on PCs, it could be useful on low-power embedded devices.

Contemporary graphical hardware, which is typically 3D-capable, does not need to resort to optimizing image transfers: it has enough memory to store images "whole", and is able to render a whole lot of them at great speed, and at any position.

Supporting these fundamentally different types of hardware requires a higher-level abstraction, so I'm toying with the idea of "virtual spaces".

For example, a listbox could be implemented as a frame that displays a range of "stripes". These stripes would be of fixed height and identified by numbers that would start at 0; however stripes could be added both at the end and before the current first stripe (and possibly inserted anywhere in-between too).

With 2D acceleration (or none), e.g. scrolling down would then translate to "blitting" the bulk of the visibles stripes upward, then triggering a redraw (delayed or not) for the newly uncovered stripe.

With 3D acceleration, the implementation would allocate textures - individually for each strip or, more realistically, of greater size so that each texture could hold several stripes -, and draw the stripes from there.

Caveat: it should be noted that using textures to store the rendered content of list box items would not normally be a worthwhile optimization - though it could be if the rendering is non-trivial. The idea of virtual spaces (1-dimensional in this example) remains valid.

### What's needed

- A draw() method on every widget. Takes into account all state and renders the widget "from scratch".

- A method invalidate() that expresses the need to redraw the widget (by executing draw() as soon as possible)

  - In a game-like rendering loop (without off-screen buffer), invalidate() can be reduced to setting a "dirty" flag.
  
  - The most common case is probably to just delegate to a callback, which is tasked with updating the UI (possibly redrawing layers both
    "below" and "above" the UI)

- All GUI renderer implementations must provide a pair of methods to prepare for and cleaning up after rendering
  
  - This does NOT mean setting the current OpenGL context, which is a platform-specific operation. This must be done by the code, directly called or indirectly triggered by invalidate(), that does the actual rendering.









































