2017-06-28
==========

Begun implementing the new approach to popups outlined yesterday. I cherry-picked this log file to preserve the reasoning that led to this attempted change.


2017-06-27
==========

I'm still trying to implement internal popups that can contain widgets of their own, and this is so much work that I'm feeling I'm doing something wrong.

For one thing, I'm duplicating code between two separate implementations of Root_widget_base - the "real" one, Root_widget, and the one attached to internal popups - and that annoys me.

The main reason for this code duplication is that the real root widget has to handle internal popups, whereas the popup one, by definition, cannot bring up popups, at least not in the same way. Yet popups are so maddeningly similar to widgets that all this code I have to write feels like horrible bloat.

Let's review my reasons why I didn't want to treat popups the same way I'm treating widgets.

- Popups are on a higher "layer"
- Popups do not participate in layouting

And I think that's it!

Yet the differences are significant. The question I'm really asking myself is whether the root widget is really the right place to handle popups. So what's the alternative ?

The only thing I can think of is the canvas itself - yet, as I've ruminated earlier, is not a solution, since a single display surface can be potentially be serviced by multiple canvas instances.

Yet again, those instances are required to communicate insofar as to share resources such as font handles. Could the same thing be done with popup zones ?

Why not - but that gives rise to the next question: which software entity will be able to abstract away the differences between "internal" and "external" popups ?

The canvas cannot do so, because there is more than drawing involved - events must be abstracted aa well. So, can the platform adapter do it ?

Probably YES - the platform adapter already provides the platform-specific base class for Window, so it could reuse that same class, or maybe its parent class, to base external popups on.

Popups, whether internal or external, would then have a similar interface to implement, which covers both rendering and event handling.

For internal popups, the platform adapter would keep a list of rectangular zones, much like the root widget does in the current attempt. It would thus be up to the platform adapter to decide whether an event should go to the normal event handling routines of the window, or to one of the popup zones instead. The canvas would be shared between the window and internal popups.

For external popups, the popups would actually be windows, for each of which the platform would create their own canvas, while also being responsible for message pumping. (It could do that alongside the main window message pumping, or in one or more separate threads.)

A detail question: how would a widget ask for a popup in this new approach ? Who would it ask it from ? It cannot directly go to the platform adapter, at least not without specifying the window. And alas, that information is lost when the window sends events to the root widget.

The only way to do that is to have the root widget maintain a reference back to the window or popup it is plugged into. This in turn suggests the definition of an interface, to be implemented by both windows and popups (both types!). "Surface", possibly ?

One downside to this new idea: more work for platform adapter implementors ? The current approach leaves it to the Root_widget class to implement internal popups, though in a manner that is not yet optimized for any scenario. Such optimization would have had to be done in Root_widget code, via preprocessor definitions.

... which leads me to a mistake in my reasoning above: not the platform adapter, but the Window class is the right place where to implement internal popups (once the link from root widget back to containing window has been established).

However, the similiraties between the old and the new approach go further - they extend into the annoying parts as well. We have, once again, the situation where a common interface (ISurface ?) must be implemented by multiple classes (in this case, Window and Internal_popup).

Can this be done via multiple inheritance ?

1) Window, inheriting from both the platform-specific base class and ISurface, and
2) Internal_popup, inheriting from ISurface only ?

No, that's not it. The way to go would be for the platform-specific window base class to inherit from ISurface, quite simply. ISurface would thus define entry points (virtual methods) for drawing and event injection. Internal_popup would simply reuse the same interface, which the Window class would use to divert input events and rendering calls as needed.

--> CONCLUSION: the current branch will probably be reverted back to before work on popups started. (Unfortunately, that's gonna axe a few smaller improvements that were made along the way.)



2017-06-19
==========

A short observation about popups - I've only just begun with the implementation, but I'm unclear about whether they really should be specified as positioned bounding boxes rather than simple rectangles.

The case can be made for widgets to use positioned bounding boxes, as the great majority of widgets contain text in some form at its alignment will most likely be facilitated by the use of a baseline and origin. But is the same true of popups ?

Horizontal alignment at the origin could help with combo boxes, vertical alignment on the base would probably help with hierarchical pull-down menus. 

I think I'm going to leave this policy in place. However one question irks me, and that's what to use as reference edge for the axis that is *not* aligned, e.g. the vertical direction for the popup under a combo box. In which situation should y_min be left and the height be attributed to y_max, and vice versa ? Does it play any role at all ?

It doesn't really, but maybe I can define one now. Let's say the choice between y_min and y_max determines the *orientation* of the rectangle on the non-aligned axis. I'm not sure what could be made with that information, but it's not necessarily useless: by consulting the orientation of its containing popup, a dropped-down (or popped-up) listbox under / above a combo box could optimize its appearance and layout without being explicitly told by its owner. For sure, that information could, and maybe should be communicated explicitly; nonetheless, the concept of bounding box "orientation" is a potentially useful one.


2017-06-17
==========

(cont'd from 2 days ago: stylesheets)

Specifying the interface:

- Widget::get_style_int(type_index, Style_element, Widget_states) -> int
- Widget::get_style_bool(..ditto..) -> bool
- Widget::get_font(type_index, Style_element, Widget_states) -> Font_resource

Is it necessary/useful to be able specify negated widget state bits ? E.g. does it make sense to define and query a state such as: non-hovered, non-focused, etc. ?

=> I don't think so. Where negations make sense, dedicated bits could be used, but I'm not sure that case will ever arise.

This means that any style bit that is no explicitly set is interpreted not as "not set", but as "undefined".

A question: where are the Font_resource objects kept, and by what entity ?

It would be a waste of CPU to have every widget look up the font handle by itself, which is why I bundled the rasterized font pointer and the associated font handle together in Font_resource. But where are these objects kept ?

A typical font style query looks like this: widget calls get_font(..). Get font will try to find a definition in its own stylesheet (if any), then climb up the container hierarchy until it either find a definition or fails.
If, say, the direct parent has a stylesheet that contains an applicable definition, should the stylesheet itself own the font resource object?

A possible issue here is with serialization. The Rasterized_font pointer is common to all possible build variants of libCppGUI, but the other half of the Font_resource class, the Font_handle, is not: its type is dependant on the Renderer implementation.

If the stylesheets deliver only the Rasterized_font pointer, there would be a need for an intermediary layer. That layer could be the root widget, which however would thus get a double role, both as a possible stylesheet source (via the container hierarchy) and in a management capacity. Not really a problem though, what's needed isn't all that much, just an Boost.Intrusive.Set of Font_resources.

The consumer would thus be given a pointer to a Font_resource instance that is guaranteed to remain valid for the lifetime of the root widget; the root widget takes on the responsibility of obtaining and releasing the associated Renderer resources at the appropriate times, so that the Font_resource objects will be ready when it comes the time to actually render text.

An alternative to this would be to assign that task to the canvas, which would dispose of the necessity to climb up the container hierarchy. Yes, I believe that option is definitely preferrable. It will require a minor redesign of the Canvas class though, which so far only caches fonts without owning them.

Another possible problem is that so far, it was not specified that the canvas instance passed to the root widget is always the same. In the current implementation, the canvas object is owned by the Window class, which is fairly safe since using that class is mandatory.

But what happens once "external" popups are implemented? Those will be based on extra windows and thus cannot share the same canvas.

This all needs more thinking through.

------

To sum up:

- Font resource objects have to potential to significantly reduce overhead when the same font is used by a lot of widgets, which is the case for the great majority of user interfaces.

- Because of that, style lookups should return font resource objects instead of Rasterized_font pointers, which would require an extra lookup (via the canvas).

- The question is where to keep the font resource objects ?

Since style lookups often follow the container hierarchy upwards, it is probably best to introduce a wrapper around the Stylesheet base class that will feature a cache for fonts. Widgets would then get assigned instances of that wrapper class instead of direct Stylesheet instances.

If ever a Renderer implementation needs instance-specific font handles, that wrapper class must, via conditional compilation, include code that supports this. Or, more likely, the Font_resource (currently defined in Canvas.hpp) must add such code; the stylesheet wrapper merely needs to be defined so that the canvas pointer is always available to be used into lookups.

Having each stylesheet wrapper instance keep its own font resource map has the drawback that a given font may be mapped more than once, which will happen when more than one stylesheet points to the same font. That is not a huge problem though.

Two questions remain:

1) When should style lookups happen ?

2) How is style lookup algorithm defined, formally ?

Style lookups must happen before layouting, or as part of that at the latest (I'm not going into style changes for now - for the time being, those are simply unsupported). The question is, should a new lifetime virtual method be introduced just for styles ?

What it comes down to, once again, is performance. There is nothing that stops a widget from getting style information over and over again, every time the widget renders and indeed multiple times during rendering. But it seems like a waste.

Options are: use a generic cache with reasonable performance, or let the widget decide whether to do caching, and if so, how.

For now, the decision is: no optimization in generic code, i.e., widget implementations are responsible; as a recommendation, the init_layout() entry points should be used for style lookups that persist their results.


2017-06-15
==========

Stylesheets
-----------

Stylesheets have moved to the top of the to-do list because their absence is interfering with even a half-usable implementation of the UI builder (which in turn is interfering with testing, which is interfering with the implementation of popups): having to specify the font for every widget separately is just a no-go.

I've already implemented a first version. It is based on the container hierarchy, and currently, ultimately defers to the root widget to obtain a font resource.

The problem with that approach isn't that it wouldn't work, but it requires a virtual method throughout the whole widget hierarchy for every single style information type. Separation of concerns takes a big hit. It would be impossible, for example, to introduce a new type of styling information for a custom widget or widget collection.

The current implementation defines a single Basic_stylesheet class that is supposed to provide all style-relevant data, which so far is a single Font_resource instance. The sort-of-idea was to let programmers derive from that class to add extra information but, like mentioned above, that would require a virtual method at least for every data type used for styling. Templated methods cannot help here, since they cannot be virtual.

An idea I want to pursue here is chainable stylesheets of arbitrary content. The container hierarchy's role would be reduced to ensuring that all stylesheets up to the root widget are consulted (if necessary) when a widget queries for style information.

The "arbitrary content" part needs more thought. I was imagining a map, keyed with: the widget class (represented by its type_index, which I've already used in my present implementation), a numeric tag, and the widget's states (Widget_states, based on enum Widget_state).

(In CSS-speak, the type index would roughly translate to the element name, the numeric tag would be the CSS class, and the widget states are akin to pseudo-selectors such as :hover etc.)

This approach could be made extensible: the type_index is guaranteed to be unique within a program, numeric tags could be registered via initialization code, while widget states could have a pseudo-extensibility by having values such as "user_state_X" in the Widget_state enum.

As with CSS, there must be clear rules to decide which rule is applied in every possible situation. For example: if the stylesheet attached to the widget itself defines a font for precisely that widget type, but does not specify a tag nor any widget states combination; and the parent container has a stylesheet that matches a twice-removed parent class plus the tag and two of the widget state bits, which of the two rules shall prevail ?

This is non-trivial, and could lead to endless headaches for both developers and designers.

Tentatively, I would say:

- More matching widget states should trump a closer widget class match; i.e. for a Textbox, if a rule for Widget says that the border color when both hovered and focused should be red, and a rule for Textbox says that the hovered color should be blue, red would win because it better matches the widget states. The ranking of the widget type match would only be taken into account when all other things are equal.

- Likewise, container inheritance should be secondary: if an applicable rule from stylesheet attached to a container matches the widget states more closesly than an applicable rule on the widget itself, still the container-inherited rule should win out.

- Tag matches should have a higher priority than both "container distance" and "type distance" (even combined). Whether or not they should have priority over widget states match is an open question.

### A simpler approach ?

The analogy to CSS should not be driven too far (for one thing, my mastery of the CSS rules is far from sufficient). The question that must be asked here is "what am I trying to achieve, precisely ?".

During my latest experimentations, my only concern was to have a default font for my text-based widgets (label, textboxes, buttons). Perhaps the above approach is too sophisticated ?

A simple approach might be for style queries to simply go up the container chain until they find a match - without even attempting any ranking at all.

What this means is that container stylesheets could no longer micro-manage the appearance of their content. However, such a system would probably be too simple to satisfy the needs - it might even break down as soon as a widget class requires two different fonts. Also, how would this work to query, for example, the color of a border if that color must vary according to the widget states?

Well, actually it could work. All that needs to happen is to let the "first match" rule play out; the only question is how to keep in control.

### Trying to come up with alternatives

So far, I have not addressed how to support the various types of style information that might be called for. I have mentioned that I would like custom widgets to add their own styles, but haven't given much thought to the mechanism needed to support this.

Maybe that requirement isn't all that important. If the styling/theming mechanism supports fonts, color, and widths, that should actually cover the basics. Plus, libCppGUI is supposed to be low-level, and doesn't deal with font names, styles, etc, nor with any other units other than pixels, perhaps "higher" forms of styling should likewise best be left to higher layers.

So, a simpler approach might no longer try to deal with widget classes at all, instead directly identifying the information that widgets need to retrieve. For example, a widget might ask for the information border_width, and obtain that information from its own stylesheet or one attached to a container higher up in the tree.

The name "border_width", in this approach, does not specify what kind of border is meant - whether, for example, for a button or for a textbox, which could be wildly different visually. That means that any time a widget needs a border width that is distinct from the already reserved "border_width", it needs to define its one style element, e.g. "button_border_width" or "textbox_border_width". That certainly lacks elegance, though it would have the advantage of forcing widget developers to get a clear understanding of their style element requirements.

Another thought. The above idea of limiting style elements to but a few (fonts, colors, widths) opens to door put styles back under the control of the widget classes themselves (very few virtual methods). This has the following advantages:

- the widget and its implementor can decide on their own whether they want to take styling into consideration at all
- if the do decide to do that, they keep the option of modifying styles - for example, they could colorize a border to give feedback for hover, focus, etc., which would help prevent styling clashes
- when no style is provided in any of the stylesheets that could be taken into consideration, the widget is free to choose one itself

So, what could this look like ?

- 3 virtual methods defined in Widget - one for each styling data type - taking the type index, the style element ID and the widget states for parameters
- optional stylesheet objects that can be attached to any widget, that the widget methods will delegate to if present

This would mean that stylesheets can only be defined by programmers, not artists/designers.

Also, this proposal does not answer the questions asked about about how to classify matches. Leaving the widget code in charge however makes it possible to consider very simple matching rules, leaving it to the widget to do its own fallbacks if a precise match cannot be found, modifying the results of such fallback matches (e.g. modifying colors to reflect states), or even combining the results of multiple fallback queries.

... and, it may also make it possible to re-introduce stylesheets with designer-defined content. Prioritizing less-than-ideal matches could even, again, be left to widget code. The query mechanism need only support masking the widget states. If the widget is capable of utilizing style elements defined for a parent class, it can specify that type index explicitly, thus keeping full control over what it will and will not take into consideration.



2017-05-29
==========

Speedbump regarding the positioning of popups. Trying to think of all angles:

- Popups can be positioned above or below, to the right or the left of the owning widget

- In some cases, flipping to the other side (e.g. placing the popup above the widget instead of below) is allowable, in some cases it isn't

  -> it could be useful if the widget could "probe" a direction without actually bringing up the popup, in order to give visual feedback to the user as to where the popup would appear (e.g. replacing the "down" arrow of a combo box with an "up" arrow)
    - [This is edge case. I think it is actually next to useless in practice.]
    
- In addition to the preferred relative position (above, below, to the right, to the left), a widget should be able to specify how to align the popup in the other direction (left, center, right, or top, middle, bottom, baseline)
  - "baseline" could be optically useful if the popup is supposed to extend an abridged text, as well as for submenus in drop-down menus
  - left and right alignment could occasionally follow the widget's own alignment
  - center (horizontal) alignment is useful if the content of the popup does not have an alignment (e.g. a video or 3D preview)

- In some cases, it might be useful to let the widget to all positioning and sizing itself - this is possible because the widget can know its own rectangle (and bounds, for the origin) as well as the extents of the root widget.

- The "flyout" is a common application of popups. It is essentially a vertical or horizontal sequence of additional widgets. That use case should be well-supported; in particular, a scrollbar should automatically be added when available space in the preferred relative position is insufficient - that feature however is one or two layers above the popups themselves.

- In some cases, it may be desirable to specify both a minimum and a maximum size when bringing up a popup (e.g. a combo box with a long list). 

- If the minimum size cannot be obtained in the aligning direction (i.e. the axis that does NOT determine the relative position of the popup), the algorithm could shift the popup. E.g. if the listbox dropped down from a combobox is too wide / too close to the right edge of the window, the algorithm could the popup to the left. 
  - This comes at a cost to user-friendliness though, by potentially forcing the user to move the mouse in awkward ways to reach the correct row or column in the popup.
    -> When using the mouse, this could be handled by "warping" the mouse. It could even be handled automatically by the code that positions and sizes the popup.
      -> However that is not allowable when the event that gave rise to the popup was a touch event. In such a case, the "shift" option should be prohibited.

- The same problem can arise with the maximum size. Whether or not shifting should be allowed in that case is a matter of preference that should be made an input parameter.

Devising the algorithm
----------------------

- Popup.place(relative_position, alignment, minimal_bounds, preferred_bounds, gap, flags) -> Bounds
  maximum_size: optional
  gap: Position_delta: defaults to 0
  flags: optional: can_shift_from_alignment, cannot_flip_to_other_side

(con'td 2017-05-30)

### Algorithm:

- check for sufficient space:

  - if placement is vertical (above or below):
    
    - if below: 
      - check:
        - client widget lower edge + gap + minimal height (= ascender - descender)
        - if not enough:
          - set flipped flag
          - check client widget lower edge + gap + preferred height (= ascender - descender)
          - if still not enough:
            - log an ERROR
            - constrain to available height
        - else:
          - if preferred height specified:
            - extend height to minimum of available and preferred
            
    - else (above):
      - check:
        - client widget top edge - gap - minimal height
        - if not enough:
          - set flipped flag
          - check client widget upper edge - gap - preferred height
          - if still not enough:
            - log an ERROR
            - constrain to available height
        - else:
          - if preferred height specified:
            - extend height to minimum of available and preferred

    - handle alignment:
      - compute left and right edges according to specified alignment:
        - left: left edge = left edge of client widget, right edge = left edge + minimal bounds width
        - right: right edge = right edge of client widget, left edge = right edge - minimal bounds width
        - center: compute middle of client widget, left edge = middle - (minimal bounds width / 2), right edge = middle + (rest of width)
        - origin: left edge = origin + left extents, right edge = origin + right extents
        (all the above should be implemented as a generic layouting routine in the appropriate module)
        - if option can_shift_from_alignment was specified:
          - if alignment was left, origin, or middle, and right edge is past canvas width:
              - shift to the left by the amount the right edge extends past the canvas edge, but no further than to canvas left edge
          - else if alignment was "right" and left edge is before canvas left edge (0):
              - shift to the right by the amount the left edge extends before the canvas left edge, but no further than to canvas right limit
        - if either left or right edge are beyond canvas confines:
          - log an ERROR
      
  - else (placement is horizontal, i.e. to_right or to_left):
    ... similar ...
  

  
2017-05-26
==========

Ruminating further on popup zones
---------------------------------

I haven't yet reached full clarity on how to implement popup zones, so I'm going to try and view the problem from all angles before starting with the coding.

In the future, the default layer and the popup zones could be rendered independently and even concurrently, if the platform driver supports that by providing separate graphic contexts. -> It should not be assumed that the default layer and the popup zones will be rendered in a specific order, or that invalidating then one will also invalidate the other, even partially.
    
Popup zones may, in the future, be implemented on the basis of separate (borderless) windows, which would typically have their own message queues (which might even get pumped by a separate thread - though not on Windows). Such popup zones will be called "external".

Regarding the main window, the (tentative) specification so far only stipulates that perform_pending_main_thread_tasks() must be called periodically. On some systems, that call may be a no-op (if the window system does not use a message queue and, for example, directly calls into the Window object).

In theory, each popup zone might need its own message pumping, which could - again, in theory - have to take place in a separate thread. That however would be an implementation detail of the platform driver, and thus does not require specification.

An open question is whether popup zones should be implemented by the platform adapter or the root widget. The intuitive answer is "both" - the root widget should be able to provide "internal" popup zones, whereas the platform adapter MAY (in the future) provide "external" popups.

It is slightly more complicated though. Even internal popups could be based on overlays, which could be a (future, optional) feature provided by the platform adapter.

What this means is that careful decoupling is required.

Event handling for internal popups:

- Input events arrive at the root widget as per usual

- The root widget determines whether the event must be forwarded to a popup zone; if so, that is what it does then.
    - Keyboard events are NOT forwarded to popup zones, at least for now (the ability for popup zones to obtain focus may be added later)
    - Mouse events go to the popup zone when their coordinates are within the zone
    - The popup zone is sent an "mouse enter" message when the mouse enters it, and a "mouse leave" message when it leaves the zone

Rendering for internal popups:

- Usually, internal popup zones invalidate like widgets do, i.e. they trigger a redraw (possibly a partial one in the future) of the window

- In case the popups are implemented using platform-provided overlays (future extension ?), the invalidation is restricted to the overlay containing the popup. In such a scenario, it is up to the root widget to obtain and manage such overlay contexts, attach compatible canvas instances to them, and call the popup's render() method.

- For popups implemented on the basis of additional platform windows, invalidation must be forwarded to those windows via the platform adapter. It is somewhat unlikely, though quite possible, that a platform-based popup will be drawn in a free or synced loop (this could be the case for example if the popup displays an animation). As per specification, it is then up to the platform adapter to ensure that the popup's render() method gets called continuously.

--- cont'd 2017-05-27 ----

It becomes obvious that popups have a powerful lot in common with normal widgets, so it may well make sense to implement them on that basis, i.e. as descendants of the Widget class.

So let Popup be a direct descendant of Widget. An instance of Popup would then act as a separate root for the popup zone it represents, though it remains connected to the "real" root widget. I'll try to review the different angles for each likely situation:

Internal popups (without overlays):
  - Architecture: internal popups are implemented as descendants of the Widget class
  - Rendering: Popup instances are rendered immediately after the "normal" children, within the same call to Root_widget::render()
  - Invalidation: gets passed "up" to the root widget, which then triggers a re-rendering of everything (standard layer + popups)
  - Input events: the root widget ensures that Popup instances get priority over widgets, but otherwise treats them the same way as widgets
  
Overlay-based popups (future):
  - Architecture: overlay-based popups are implemented as descendants of the Widget class
  - Rendering: popup instances are rendered separately, by the platform driver calling Root_widget::render_overlay()
  - Invalidation: done by calling invalidate_overlay() on the root widget, which will forward to Window::invalidate_overlay()
  - Input events: same as internal popups
  
"External" popups (future):
  - Architecture: the client widget class derives from Popup_window (which derives from a class implementing the Platform_popup_window concept)
  - Creation: the client widget passes itself to the constructor of its Popup_window descendent; the inherited constructor of the Platform_popup_window implementation registers the instance so it can pass events to it
  - Rendering: the platform driver is responsible for calling the render() method on the Platform_popup_window-derived object
  - Invalidation: via the invalidate() method of the Platform_popup_window implementation
  - Input events: the platform driver injects events directly into its Platform_popup_window implementation, as it does for normal windows

All these variations have in common that they are not functional (concrete) widgets, nor containers of such. The need for popups that can contain widgets is fulfilled in another step, one that abstracts away the differences between different popup types: Root_widget::create_container_popup(...), which returns an object that is derived from Container_base that serves as the root of a widget tree that is similar to, but separate from the root widget.

---------------

Implementation plan
-------------------

### Step 1

- In the test application, create a custom widget called Popup_test_button, which will drop down (or up, depending on the position) a rectangle that will be filled with a uniform color (let's say light yellow).

Implementation
--------------

Roadbump: trying to derive Internal_popup_zone from Widget revealed the problem that the layouting methods (init_layout(), get_minimal_bounds() and layout()) are not needed in popups; and in fact, there are quite a few more that are equally unneeded.

This now needs careful thinking over. The first impulse would be to factor out the methods common to Widget and Internal_popup_zone into a base class that could be called GUI_element or similar, which offers only a minimal interface, consisting of init() / cleanup() (to manage graphical resources), render(), and the event injection methods.

Thinking about it, this seems the right way to go. It does not stand in the way of the next step, the container popup; actually, it makes the design clearer.

...

Done. I've factored out about half the members of the Widget class into the new UI_element class. Some points of note:

- UI_element can NOT be a child of a container (class Container_base) (no container() property, no is_first/last_child() methods)

- Consequently, UI_element does not have a back reference to a container either, which also means it cannot find the root widget by walking that chain up.

- As a further consequence, UI_element does NOT participate in the focus handling, since that is based on a container "chain" from the root widget on down.

- All events are injected at the UI_element level, though the handlers are all empty and return false (= "event not consumed")

- UI_element does introduce the invalidate() method, but as a contract only. It must be implemented by derived classes - in particular, the Internal_popup_zone class I had begun writing.



2017-05-25
==========

ComboBox
--------

As the next step of libCppGUI, I'm thinking about implementing a Combo Box.

Combo boxes introduce a new requirement, the ability to overlay the drop-down listbox over the rest of the UI.

A bit of research shows that, in the case of Windows, that capability goes so far that the drop-down box can go outside the containing window.
While this would be doable via an extension to the platform driver, I'm not sure it's the right way to go, at least for now. I'm going to introduce instead a "popup" layer, which is going to be a function of the Root Widget.

The Popup Layer overlays the root widget rectangle exactly, but the root widget ensures that it is drawn on top of the "default" layer.

I'm unsure whether that layer should participate in input event handling though. If yes, that would mean that any kind of widget could be put there - which would beg the question of how that would work should someone put, for example,  a combo box there. That would almost inevitably mean multiple popup layers - added complexity that I have no enthusiasm for.

But if the popup layer does not accept widgets, what does it accept ? Here are the possibilities:

a) It does not accept anything at all and can in fact only be drawn upon.

b) It accepts full widgets, with the requirement that those widgets will be "remote-controlled" by the creator widget and never handle input events on their own.

c) It accepts display elements that are widget-like, but do not have the full interface of widgets.

I initially liked option (a) most, because it seems the cleanest approach: everything stays under control of the widget that creates a popup. The popup interface could create extra rectangular regions, which would be overlayed over the root widget's default display rectangle and be painted via a separate method Widget::redraw_popup().

It gets a bit more complicated because of input events. The root widget should forward any mouse events occurring over any popup region to the originating control, which would then have to keep track of the position and size of its popup(s) to determine how to interpret such events.

This could be a bit of a bummer: in a future extension, popups could be implemented such that they are in fact extra, borderless windows (allowing them to extend beyond window confines). In such a scenario, the root widget would then need to "anonymize" mouse events before sending them to the widget - which would then most probably have to do the opposite, i.e. find out where they actually occurred. This could perhaps be tackled by introducing one or more extra parameters to input event handlers (breaking change, but at this point that doesn't matter so much).

I realize now that the "popup layer" need not, in fact, be a single layer, as that may not actually make things simpler.

It is an open question whether or not multiple controls should be allowed to use popup layers (perhaps popup "zones" would be a better term). I would tend to answer "no", because there would be no non-complex way to ensure that multiple popup zones do not overlap, which would probably be disastrous.

EDIT: it occurs to me now that I forgot about "hint strip" - those floating rectangles of text that get overlayed over listbox items upon mouse hover when the items are too wide to display within the confines of the listbox (there are other varieties). In such cases, inter-popup overlaps are very unlikely.

To sum up the reasoning so far:

1: the interface does not expose a popup "layer", but allows creation and management of popup "zones" or "regions"
2: popup regions can be rendered into, and they collect pointing device / touch input, but cannot contain widgets or other objects

So, how are widgets going to work with these popup regions ?

Usually, widgets will only have popup regions as long as they have keyboard focus - there may be exceptions, but regardless of such details, popups are intended to be transient. Their presence would otherwise disrupt the functioning of the rest of the UI because it hides parts of it. (This is just an introductory thought.)

The switchboard for popup regions should most probably be the root widget, as planned. Its interface for that purpose should have the following capabilities:

- Find a suitable position according to the size and alignment criteria of the requesting widget
- Inform the widget of the bounds of the chosen rectangle (which should be specified relative to that widget's origin)
- Redirect mouse / touch input events occurring on a widget's popup(s) to the widget [Container_base::child_at()!]
- Cleanly remove the popup 

Looking at the existing code, I now realize that trying to get the system to "pretend" that a popup is a part of the widget itself may not be a very good idea. The problem is that current code is relying on the fact that widgets are invariabley of rectangular shape. It would be possible to remove that restriction, but only at the expense of adding significant complexity.

There is a better way: by defining an interface "IPopup" and having the root widget forward relevant input to that, we preserve the simplicity of the rectangles.

IPopup entry points include:
- redraw()
- mouse_motion(), mouse_button(), mouse_click(), mouse_wheel() (similar to Widget)

It is up to the classes implementing IPopup to appropriately coordinate with their owning widget to implement these entry points.

The root widget offers the following facilities for popup creation and management:

- show_popup(Widget *owner, IPopup *popup, Extents min_size, Positioning, Alignment, int margin, uint32_t flags) -> int id
    
    - "Positioning" specifies whether the popup should preferrably appear to the left, top, right, or bottom of the owner widget
    - "Alignment" specifies how to align the non-constrained axis
    - "margin" specifies the distance between the widget and its popup
    - "flags" could be various options, like whether or not the root widget is allowed to "flip over" the other side if space is insufficient on the desired side (Positioning)
    - "id" is an opaque, non-zero number by which the root widget knows the popup
    
- hide_popup(int id)

Most probably, the abstract IPopup interface will be replaced with an abstract class called Popup that can take care of some housekeeping as a bonus.



2017-05-06
==========

The SDL2 dilemma
----------------

So far, I've use the git submodule functionality together with CMake's add_subdirectory() to provide libCppGUI with its dependency SDL2. This worked well, but has revealed a fundamental problem.

That problem is that the targets defined by the SDL2 CMakeLists.txt are only valid "downstream" and on the same level or lower of the CMake project tree. More specifically, the CMake file of SDL2 uses include_directories() to add a path to the SDL2 include files directory, which makes that path available to libCppGUI because libCppGUI and SDL2 are siblings in the project tree; the consumer of libCppGUI (the application program) however does *not* get that directory added to its include file search path, being higher up in the project tree.

This could, alternatively, be interpreted as a bug/deficiency in SDL2's CMakeLists.txt. Adding the line 

  target_include_directories(SDL2-static PUBLIC $<BUILD_INTERFACE:${SDL2_SOURCE_DIR}/include>)

to the definition of the SDL2 library (static version, but should work the same for the DLL) makes the include directory available to all consumers, no matter their position in the CMake project tree.

I've proposed this change via a pull request to spurious/SDL-mirror - let's see what comes of it.

I should note however that I'm rather unhappy with the way add_subproject() adds myriads of unwanted, internal targets to the Visual Studio solution. I'm going to have a closer look at ExternalProject_Add().

------

Well, having looked at ExternalProject_Add() for a bit, I can't say that I'm thrilled by it. It is apparently intended as a way for the build system to obtain dependencies on demand, but because those dependencies are only built when the project itself is built, CMake's file discovery mechanisms (config files and find modules) do not work, leaving a lot of manual work to the programmer - with potentially disastrous consequences for portability, since a programmer will typically only be able to make it work on his own machine.

It seems to me that ExternalProject_Add() is a lower-key attempt at doing what Conan was created for. So, I'm now going to re-enable Conan and use that to satisfy the SDL2 dependency, and also perhaps freetype (with zlib) while I'm at it.

As an exercise, I could also try to make Conan an option - but then I'd have to provide a find module for SDL2, freetype and zlib. 



2017-03-09
==========

- Button and Glyph_button both need to have a way of handling descents, and lack thereof


2017-03-08
==========

- Move Box functionality into Widget (currently separate class) ?

- I removed the x_min adjustement from Canvas::render_text() - it was messing with the new, baseline-oriented layouting approach. It is quite possible that text alignment (especially to the left) will now require special care.


2017-03-06
==========

- Note: support canvas-specific OpenGL context (separate from the context of the window) so that canvas does not to manage state

- The OpenGL renderer displays text correctly when the Y axis is oriented downward, but incorrectly in the opposite case.


2017-02-09
==========

Quite some development has happened after this point, but that turned out to be a dead end. If this library has any chance of becoming sustainable, it needs to be simplified.

This means disposing of:

1) aspect-oriented (or -inspired) programming: it has proven to lead to code that is too complex to handle

2) keeping the layouting-code optional (via aspects): unworkable for the above reason, though it's conceivable that it might be reintroduced at a later time via conditional compilation

My intention is to start with Widget.hpp and work my way down until the consumer project (LSInstructor) builds and works again. I will keep notes here as necessary.



2016-07-09
==========

23:35
-----

I completed the first phase of replacing the box model concept, and the test app compiles once again and produces decent visuals. Some work however remains to be done:

1) The current implementations of the box model concept do not yet separate out the layouting functionality into separate (sub-) aspects.

2) For the runtime variant, there is currently no way to define default values.

    [I have begun working on this, but didn't finish yet. One thing that needs to be done is to apply the CRTP pattern to `Container_base` (and probably to all `..base` classes), so that `Box_model` get access to overloaded default values defined as constexpr members of the implementation class. 
    One variant of this would be to turn `container_base` into a standardized aspect. ]

2016-07-08
==========

21:44
-----

The first step towards making libCppGUI capable of working with a future designer app consists in simplifying the box model along the lines I've outlied earlier today.

This means:

- A box model template that is parameterized with, on top of the standard parameters Config and With_layout, a boolean that indicates whether the box model will be "dynamic" or "static" (those parameters go to the outer struct, i.e. the aspect "wrapper"); and the aspect template parameters according to the updated standard, which uses a CRTP `Class` parameter on top of the `Parent` parameter used for aspect chaining.

- Two specializations, one for dynamic, one for static configuration.
  - These two specializations could (should?) inherit from a common that provides common functionality, such as rendering.

- A layouter aspect that can be injected into the box model template. That one does not need the dynamic/static parameter, as it will simply work with whatever the box model main aspect provides.

Also, the `Box` intermediary class, which used to provide box-related functionality based on the data provided by the `Box_model` implementations, should no longer be required, as by definition, the new `Box_model` will work with standardized box model properties.


--------

A new challenge is appearing on the horizon, in the form of an expressed need (desire ?) by a customer to be able to design user interfaces manually - as opposed to programmatically, as I was hoping I could convince him would be good enough for the time being.

This desire is "inconvenient", because a) writing a designer application would be a lot of work under the best of conditions, and b) the whole architecture does not feel very structurally stable at the moment, increasing the risk of having to rewrite big parts of such an application in the not-so-distant future.

One very specific problem mandates careful assessment right now: the fact that I have tried to make as many parameters as possible "build-time" by defining them as parameters of the class templates. This approach, of course, is almost diametrically opposed to any attempts at run-time configuration.

The problem stands out most prominently with the box model. I had planned on creating many templated implementations of that concept, practically all of which taking template parameters for border width and color, margins, and inner padding. I have already created a few implementations that were using the same values for all four cardinal directions, and was planning on adding implementations that could have direction-specific values. I had also vaguely planned to create implementations that could be parameterized at run-time.

However, I see no practical way to make a designer create template instances with varying parameters at run time, short of invoking a build chain to generate code on-the-fly and then linking it dynamically. While such an approach would be interesting and might have applications more interesting still, I do not have it at my disposal right now.

[Thinking about this further, coming up with a minimal such system might not be quite as difficult as it sounds. CMake could be used as a convience to handle platform specifics. However, this leaves open the question of how to (dynamically) link such code with the designer application. Because the designer app cannot instantiate the templates, it would need to access widgets via an abstraction layer.
All in all, this idea should not yet be discarded entirely, as it may yield long-term benefits.]

The only alternative I can see right now is to take a step backwards and to move customization back into the runtime domain. In order not to completely loose the ability to optimize at build time, I would need to come up with a reusable pattern that allows parameters to be interchangeably build or run time.

This would mean that for each customizable template, a boolean template parameter should be added that determines whether or not customization will be build or run time.

When run time customization is specified, template specialization will inject the necessary properties for the values to be set programmatically.

I'm not too sure what to do if compile-time customization is chosen. So far, I've implemented all customization via template parameters, but I'm not sure that is the best or even the correct way of doing things.

Thinking aloud here... what about doing the same thing as for run time customization, but with constexpr getters only (no setters) ? This would require the code to be universal, as optimizations via specialization would no longer be available.

How would user code that customizes a widget (or aspect thereof) actually look like ? CRTP could be used to access the getters, so that injection could be written as simple getters in the derived class (with defaults provided in the class/aspect itself).

The question remains how a widget customized at run-time should be "solidified" into a compile-time customized version when the UI code is being generated - and whether it is worthwhile supporting this at all.

It would involve generating a derived class for each combination of parameters that are being used in a given UI. This certainly seems possible. It would involve using the properties metadata - which is needed anyway for the benefit of the designer app - to generate a getter for each property.

Another question is how exactly to implement properties. I think the key here is to make use of the already existing "aspect" mechanism, which ensures that all parts of a class (or at least, all parts that can have properties) share a single line of inheritance (multiple inheritance is restricted to injecting property-less facilities).

`Widget<>` or `Abstract_widget<>` defines a static constexpr method `metadata()`, which returns a data structure of type `Metadata`. `Metadata` would be composable, making it possible for any derived class or aspect to override the inherited implementation of `metadata()` with its own, which calls the inherited one and adds its own metadata to the result.

Such metadata should include: the name of the class or aspect, and a list of property descriptors. Each property descriptor should include: the name of the property; its type (string, boolean, float, or enumeration).

One question remains open: how to work with widgets or aspects that can be customized with data types, one example being `Slider<>`, which could benefit from taking the type of its value as a parameter, which would allow it to support integer, floating point, and even rational numbers with the same code base. I think the only practical way to support this is to hardcode all sensible choices into the designer application, which implies that any widget class has at most one such template parameter.


2016-06-30
==========

- There is a problem with focus cycling


2016-06-11
==========

A plan is needed to (already) overhaul the new Layout Managers. The problem is that they are now dependent on a specific type of layouting data (padding + spacing), which a) may not be enough in all cases, and b) is sometimes not needed at all.

### Idea 1

One possibility would be to handle the possible extra layouting parameters by overloading the layouting methods, e.g.: 

- `layout(Container_base &)`
- `layout(Container_base &, const Padding &)`
- `layout(Container_base &, const Padding &, Width spacing)`
- `layout(Container_base &, Width spacing)`
  
[and similar for get_minimal_size()]

This would put the burden on the Layouter aspects of specific Container_base descendants to invoke the layout manager with the right parameters.

### Idea 2

Another possibility would be to again overload the layout manager methods, but this time with specialized Container descendants:

- `layout(Container_base &)` (no padding, border, or spacing)
- `layout(Container<Config, true, Bordered_box> &)` (border only)
- `layout(Container<Config, true, Bordered_padded_box> &)` (border and padding)
- `layout(Container<Config, true, Bordered_padded_box_with_margins> &)` (margins, border, padding)

The advantage is that the Layouter aspects no longer have to adapt by themselves. 

Another possible advantage is that fallbacks are possible: a layout manager that does not explicitly support specialized containers would still support the "naked" Container_base. Though that is probably not worth a whole lot in practice, as all layout managers would have to be support all varieties eventually.

### Idea 3

Yet another idea is to define the Box concept in such a way that the layouting algorithms can stay identical whatever box model is being used. This could be achieved by defining methods on the Box concept implementations that abstract away the differences between those implementations, such as:

- `wrapping_width(border) -> Width`: returns the distance between outer and inner rectangle for the given direction (N, E, S, W), OR 
- `inner_rectangle(const Rectangle &) -> Rectangle`, achieving the same goal in a different way ?

Those methods would typically be inline and static and/or constexpr and thus wouldn't incur any real runtime cost.
As an added bonus, layout managers could still provided specialized implementations should the need arise.

----

Clearly, idea (3) is the most promising.

Something else is now bothering me about this, it's that layouting managers as currently defined do work that could be used else where, specifically to do the internal layouting of buttons and other types of "boxed" elements. Thinking about it though, there are more differences than similarities, and the work is not complicated anyway. The best way to think about this is probably to add some conviences routines to the Box concept anyway, most efficiently by defining a common CRTP base class, e.g. `Box_model`.

Correction to the above. In fact, with the current concept, the box model will have to be injected at different places in the class inheritance graph: somewhere below Widget<> for non-container widgets, and somewhere below Container_base<> for container widgets.

This means that either

  a) `Widget<>` methods making use of the box model (e.g. `draw_borders()`) need to be given a reference to the specific box instance they are supposed to work with, or
  b) Box model-related methods be injected below `Widget<>`

The combination of (a) and (b) would be to derive helper classes from Box_model specializations that know about `Widget<>` and can be easily injected at any level.

It would also be conceivable to inject the box model as a component (HAS_A) instead of an ancestor (IS_A), which however leads to the usual problem that empty structs cannot be optimized away from the memory layout.

In fact, that problem is the reason why this project's "pseudo-aspects" were developed, and indeed the `Box` functionality should be treated as an aspect.

So, we need:

- a `Box_model` concept
- a `Box` aspect, parameterized with the `Box_model` implementation (though it may take on additional parameters later on)


2016-06-10/11 (after midnight)
=============

I tried to simplify the Container hierarchy today and removed the Container<> class, rewiring things so as to keep the compatibility (as far as possible). That introduced a few bugs, which still need to be resolved.

Part of what I've done was to have Root_widget derive from Container instead of from Abstract_container. This may have been a mistake, as Widget includes the ability the be a child of an Abstract_container, which Root_widget does not need. I will probably have to undo that change.

Because of the above mistake, I had considered moving the invalidation locking functionality to the Window class. It may still make sense to do that.

Another thought is that the Window concept implementation should provide compile-time information about the way updating should work - though that may also depend on the graphics adapter, so the exact mechanics are still subject to more thinking and planning.


2016-06-08
==========

- Is it legal, or should it be made legal, to update a widget (e.g. call Textbox<>::change_text() ) before that widget has been "initialized" (method init(), better term and method name still pending) ?

 
2016-06-08
==========

- Stringlist needs to visually reflect the fact that it has focus.
- Stringlist does not take focus when clicked on items (scrollbar does take focus though)

- I really need a better alternative to assert()

- TODO: move the layout managers into a subdirectory, with a file pair for each


2016-06-07
==========

- Stringlist (and other containers with specific layouting) should very probably inherit from Container_base<> instead of Container<>.
  -> [now done for Stringlist<>]


2016-06-06
==========

- TODO: check: when container triggers mouse_enter() on child, is mouse_motion() still being relayed to said child ?


2016-06-02
==========

- Should items be selected upon mouse button down or mouse button up ?
  -> clearly: down!

2016-06-01
==========

- When scrolling the Stringlist (or a Scrollbox) and the mouse pointer was resting on an item, highlighting it, that item stays "hovered" even though it no longer is. I "fixed" this for the Stringlist by manually resetting the hovered item index to -1, but this is clumsy, and the Scrollbox derivates still have the problem.
  To really address the problem, any "scrolling" operation (the corresponding methods are still not defined yet, still using standard invalidation as placeholders) should also trigger a re-evaluation of the mouse position.
    -> however, in the case of the Stringlist (NOT the Scrollbox family), it would be sufficient to just apply the item delta to the index of the hovered item.
    
- The UI seems sluggish. I don't know why that should be the case; the time may be ripe now for some profiling. Maybe Resharper can help with that ?


2016-05-26
==========

A few TODOs emerged from today's cleanup work (aimed at creating a reusable skeleton project - without success so far).

- The Icon_resources module (templated struct) gives access to binary (pre-rasterized font) data. Looking at the compiler warnings today, it turned out that the fact that the accessor methods are constexpr makes them automatically inline (of course - in hindsight). 
  The data returned by the accessors is obtained via a header file, called "all_icon_fonts.h", which is generated at build time. Unfortunately, `all_icon_fonts.hpp` doesn't just declare that data, it actually inserts it into the source via `#include` directives.
  This is wasteful in terms of build times. What needs to be done here is split that file into a declaration and a source part, which latter should of course take the form of a `.ipp` file since font and glyph sizes cannot be known before building the library consumer.
  
- Regarding the same module, it turns out that grouping all "icon resources" into a single "provider" templated struct may be suboptimal. The reason for that is that different "icon" glyphs are consumed at different pixel sizes, which will lead to wasted const data since the current `CMakefile.txt` code rasterizes all of the requested glyphs for each requested pixel size.
  It would certainly help to upgrade the CMake code that rasterizes and converts to hex the icon glyphs. But when doing so, the opportunity should also be used to upgrade the `gpcbin2c` utility: instead of just generating comma-separated hex codes and leave it to the programmer to use those to generate constant data, the utility should be able to produce self-contained header and source files on its own, giving the user a choice between standard C arrays and C++ `std::array`s for more flexibility.
  Also, the functionality of `gpcbin2c` should be made available as a C++ class, so that `gpcfontrasterizer` can be upgraded to generate source code on its own, which would greatly simplify the CMake setup.


2016-05-24
==========

Some important changes today.

- I've reverted back to using implementation files. The compilation times were becoming unacceptable, and I had misunderstood the way explicit template instantiation works - it has no effect on inline methods, and all methods implemented inside a class declaration are automatically inline.

- I've moved the user event injection methods from Abstract_widget<> to Widget<>. Since only Widget instances can be put into containers, there is no sense injecting at a higher level (the implementations were empty anyway).

- I've also removed mouse_click() from the injection interface, and added a "clicks" parameters (of type Count) to mouse_button(). mouse_click() still exists, but it's now generated by Widget<>::mouse_button(), no longer by GUI_window.


2016-05-23
==========

Today's work has revealed a couple of fundamental design questions, to which I have provided answers that need to be considered provisional.

The most important question is whether or not, or more probably exactly how and to what extent, user code may change the user interface from within event handlers.

The provisional answer I gave to that is: "yes, you may"; but I've already had to fix the mouse_click() handler of Button<> to make this work, and more correctional work may be needed.

The fundamental problem burns down to this: if event handlers are allowed to modify the user interface, then methods triggering events must allow for the possibility that their object instance ceases to exist while the event handler executes, and it would therefore be illegal to access any methods or fields after the event handler returns. Even worse, if multiple event handlers are going to be supported in the future, the dispatch mechanism would have to provide a way to stop subsequent handlers either from being called or from accessing removed widgets.

The problem could be further compounded by the possibility of implementations to have the UI be rendered in a different thread - something that, so far, is not accounted for in any way, but could be necessary in the case of, say, a game engine that uses a separate rendering thread.

It would probably be best to define a locking method that guarantees that the UI can be modified safely.

- When the platform renders the UI inside the same thread that also responds to input events, no synchronization is necessary.

- Even when no locking is necessary, debug builds should still guard against as many potential bugs as possible (such as user code asking for a UI access lock more than once, or libCppGUI code trying to access widget members after an event handler has had UI access)

- When the UI is rendered in a separate thread, access must be protected by a real mutex.
  
- Alternatively, UI-modifying code could be deferred. This would involve using callbacks (usually lambdas). For such a solution to be efficient, compilers would have to be able to optimize lambda expressions that are called immediately - which according to an experiment I just conducted on http://gcc.godbolt.org/, they are not. Perhaps a solution could be devised with the help of a couple of macros.

2016-05-11
==========

There are open questions concerning the relationship between layout() and init().

The fundamental problem is that layout() is designed to be called before init(), yet can also be called afterwards (such as after a resize of the containing window).

So far, I've ignored the implications - but they do exist. For example, the thumb of an already operating scrollbar will need to be updated after a reflow, both because the scrollbar itself has changed in length, but also because it is very likely that the scrollable it is connect to has changed too.

What is needed here is an additional entry point in the Layouter aspect hierarchy, to be called automatically from init() but also after a reflow. The definition of this entry point is something like "compute view from data" (e.g. for the scrollbar, the data is the position that it represents/controls).

compute_view_from_data() should be a virtual function defined in Abstract_widget<>. Most existing widgets should be updated accordingly.

Addendum:

I had some confusion about the right time / place to call Root_widget<>::init(). This was owed to the fact that init(), before now, had two distinct responsibilities: to connect the GUI elements with the backends, and to compute the views. Because layouting was kept separate, I had so far missed that latter part because it appeared to be a small one (which is actually not true, because the Textbox at least needs to do quite a bit of stuff there).

Having cleared that up, it becomes clear now that init() must be implemented in such a way as to be completely independent of the layouting and other view-related parameters, so that it can be called *before* run-time layout().

Actually, it might be a good idea to rename init() to something more explicit, such as connect_backends().

2016-05-09
==========

I let myself get bogged down by over-thinking this stylesheet business. I'll try to recap before letting the matter rest for now.

- The management of font handles is probably implemented in the wrong place right now, i.e. in the root widget, when it acually belongs with the Canvas (which might delegate it further to a "resource holder" if the graphics resources should be shared with other Canvas instances)

- At this time, colors are described statically, so there is no need for member variables to hold their untranslated values; and since translation is trivial and build-time (constexpr), there is no need to store the translated values either.
  - This will change when stylesheets are introduced. It will make sense then to store the translated value at "style application" time (no need to cache the untranslated value).
  
So, even though at the current time there are resources (i.e. colors) that require neither the untranslated nor the translated value to be store, this situation is temporary, and there is little point trying to use the "members-via-inheritance" trick to work around the waste of space that would otherwise be introduced by the "no zero-sized data members" rule of C++.

What is required though is a specializable template that is capable of holding either untranslated, translated or both handles of a given resource, with hooks for init() and cleanup().

I'm going to spend the next couple of hours implementing this.


2016-05-06
==========

My thinking process regarding stylesheets does not seem to be making real progress. I'll now try to come up with a pragmatic approach that will enable me to continue without getting bogged down by a big re-design. What do I need ?

- Styling parameters (colors, border withs, font parameters) must be defined outside of the code itself (there are many such stopgaps in the current code)

- Some of these parameters must be adapted for use by the subsystems (graphics, sound later on)
  - This happens during init()
    - init() is therefore the opportunity to "kill two birds with one stone": 1) obtain the style definitions (which could be a somewhat involved process once stylesheets finally come into play) 2) obtain the adapted resources (primarily fonts)
      - It also means that style changes would require a cleanup() (yet-to-be introduced opposite of init()) followed by a new execution of init(). This actually looks like a rather elegant compromise (between full CSS-like freedom and lightweight implementation)

- Inheriting style definitions from containers will have to happen, though not necessarily now.

- The same is true of run-time assignable stylesheets - though actually, this is a feature that would usually not be needed at all and that I therefore relegate to low priority.

So, the way forward for now looks like this:

- All style definitions are moved to inheritable methods.
  - These methods can be static or constexpr for now.
  - Later on, these methods will redirect to stylesheets [actually, the support for stylesheets could be encapsulated into aspects, of which there could then be variants with and without stylesheet support]

---

Addendum: self-aware structs (http://duriansoftware.com/joe/Self-aware-struct-like-types-in-C++11.html, though the "self-aware" part may not be needed) might be of use to access resources without caring about whether or not they need adapting (works around the C++ limitation of not supporting 0-sized structs):
  - for each resource:
    - define a struct that has entry points for init() and cleanup() plus an accessor get()
      - init() and cleanup() can be no-ops if the resource does not need translating
      - get() will either be a pass-through or return the handle of the translated resource
    - have the widget class inherit from that struct OR put it into a grouping struct
    
If resource structs are grouped, meta-programming (apply()) can be used for initialization and cleanup


2016-05-05
==========

About styles, again
-------------------

- Is it really necessary/desirable that they can be changed at run-time ?
  - Possible "middle ground": do not allow switching at run-time, but re-generate the UI (possibly using persistency to minimize the impact on the user) ?
  
- Is it desirable to make stylesheets static ?
  - PROs: less run-time costs
  - CONs: types are no longer identical -> unnecessary polymorphism (with an extra inheritance level) would be required to "hide" differences
  
=> Design decision: stylesheets are consulted at run-time (not specifying when exactly yet)

- How / when are stylesheets applied ?
  - Because they can influence layouting, stylesheets are needed when layouting (which may be at design-time or at run-time)
    - This may also mean that some style definitions are not needed in layout-less instantiations, such as padding
    
- Should styles really cascade ? Does it make sense that e.g. the button border width may be defined anywhere in a hierarchy ?
  - While this makes sense in the browser, I'm not convinced it serves a real need in a standard GUI.
    - Example 1: A property panel embedded in a larger UI, where properties are displayed with a smaller font to save space.
      - the font size need only be known at layout time
      - the canvas font handle (NOT the rasterized font) should be inherited from the panel
        -> individual widgets should probably cache the font handle so they don't have to climb the hierarchy to get it
  
- The content of stylesheets should be computed statically, so that the compiler and linker can throw away unneeded stuff.
  - But in order to avoid polymorphism either via virtual methods (run-time bloat) or template parameters on the consumers (which would introduce type incompatibilities between otherwise identical components, see above), the structure of the stylesheets needs to be normalized
    -> Templatized factories producing normalized stylesheet objects (via a constexpr factory method) ?
    
- How to make stylesheets extendable, e.g. to support third-party widgets ?
  - Relatively easily: by deriving. The problem however is when multiple third-party extensions are involved that do not know each other.
    - Each widget defines a struct with the run-time styles it needs
    - 
    
... trailing off ...
  
2016-05-03
==========

A thought about borders: I think I should change the way I used to think about them. Borders are NOT a universal concept to be applied everywhere - I guess that sort of thinking came chiefly from my experience with CSS.

Instead, borders should be used where needed, and only there: around text input fields ("textbox"), list boxes, "combo" boxes, etc. (in fact the presence of the word "box" is a clear indicator of where borders are of use).


2016-05-01
==========

I have made good, if slow, progress on the Listbox these past days. A few things are worth noting:

- take_focus() now propagates upwards, in the sense that if a widget refuses to take the focus, it will still inform its container, so that the container, in turn, may take the focus upon itself (which it should do anyway to put itself in the focus chain).

- There is a need to have visual feedback clearly indicating which widget has the focus. In particular, it is annoying that, even though a listbox can now be (indirectly) focus by clicking on its scrollbar (or any of its children, though with a list of buttons, that cannot be done without clicking one of the buttons), there is no indication that the listbox is now ready to be navigated with the cursor keys.

### Other things to think about

- Exception safety:
  - When is it ok for a widget to throw an exception ? 
  - Can it be ensured that the GUI will stay in a stable state after an exception was thrown ?
  - Callbacks should be upgraded to real events
    - Is there a motivation to use aspects to support both simple callbacks or full-blown signal/slot handling ?
  - What about concurrency ?
    - Must all operations on the GUI be serialized, or is concurrency allowable (and useful) in certain cases ?
      - e.g. a worker thread reporting progress via a progress bar ?
        - typically, the worker thread might change the value of the progress bar, then call invalidate() which would be responsible for queueing a redraw by the main thread
      - loading an image (e.g. for a supposed image viewer widget) and preparing that image for use by the graphics subsystem could be done in a worker thread IF the graphics wrapper supports this (e.g. with OpenGL, a secondary context could be created). If however it does not, then the graphics wrapper should provide a means to bring that operation into the main thread transparently (doable via a lambda, which the compiler can (hopefully) optimise into inline code when deferring is unnecessary)

- States in debug builds ?
  - It may be useful at some point to protect library users (or developers, just as much) from avoidable mistakes by enforcing (in debug builds only) discrete states, with all methods being associated with one or more of those states and only valid for calling when the widget (or entire GUI) is in one of those states. E.g.:
    - defining the user interface (through set_...() methods) may only be done when in "setup" state
    - the "layouting" state is only available if the library is configured to carry that aspect
      - However layouting can still occur for specific widgets if the aspect has been specifically enabled on them. In such a case, however, "layouting" would be a secondary state and concurrent to the GUI's "main" state.
    - init() will put the GUI into "initializing" state (and only be allowed following the "setup" or the "layout" state)
    - event handlers require the "ready" state
      - ? is there a way to prevent loops in event handling ?
        -> probably yes, but not with a state machine - it might involve a lot of "friend" statements instead
    - rendering requires the "drawable" state (which can only be entered via the platform adapter) (actually, the exact interplay between the platform adapter and the library proper will need some thinking about)
    - some states could be thread-local, while most would probably be global (to a given root widget, or even to the whole process)
  
  - Another thought: all adapters (graphics, keyboard, mouse, sound, etc.) should define a method them of state changes. Of course those methods are only allowed to carry out checks - they musn't do any work required in release builds because they won't be called then.
  
  - The state machine would most certainly "live" in the root widget.
  
2016-04-28
==========

First implementation attempt of the Listbox failed (as in "almost worked").

Working on it revealed a problem I had forgotten about: the fact that the thumb cannot usually be perfectly synchronized with the content area it is linked with.

It is therefore important, when moving the thumb by dragging it, that the thumb *movement* be used as input, and NOT its position!

It may also be helpful to "normalize" the thumb's position after a dragging operation has ended - in fact, that would probably be the only way to ensure that the thumb can reliably be moved to either end position.

It may also mean that implementing the Listbox as a specialization of Scrollbox was a bad idea. Though the current implementation of the Scrollbox only has a vertical scrollbar, that will change in the future, while a Listbox usually does not have a horizontal scrollbox; in fact, it would be possible to compute the minimal width of a Listbox on the basis of the widest of its contained items, all but removing the remaining similarities.
  
  -> Q: is there a legitimate use case for a list box with a horizontal scrollbar ? -> YES, thinking about it, this is quite possible
  
Therefore, is it still possible and a good idea to implement Listbox as a derivation of Scrollbox ?

I think the answer is YES: all it really takes is a way to customize the navigation, i.e. to leave it to the concrete implementation to decide by how much to travel when one of the buttons is pushed, the mousewheel is used, the slide range is clicked either before or after the thumb, or the equivalent action is performed via the keyboard.

Touch-based navigation, implementation of which hasn't started yet, is another incentive to have a single implementation.

Instead of derivation, how about using composition ? Let the scrollbox ask its content pane how far to travel, while to scrollbox still does the travelling itself ?

  -> Might work, but what about selecting/focusing/highlighting items ?
    
    -> This must be done using a "bring into view" mechanism, which is quite distinct from scrolling itself:
      - Scrollbox gets input (scrollbar, keyboard) for "up/down one item"
      - Scrollbox sends that input along to its content pane
      - Content pane changes its "selected item", notes its position
      - Content pane tells container scrollbox to "bring into view" rectangle of newly selected item
  
About travelling:

  - With a standard scrollbox, the travelled amount is arbitrary for "single step", while the amount for "page up" / "page down" is simply the visible size (= the inner rect of the scrollbox), possibly minus a configurable "overlap".
  
  - With a listbox, the "single step" is the step from the beginning of the first visible item to the beginning of the next one.
    
Possible approach:

- Scrollable_pane as a specialization of Container, adding the ability to communicate with the scroll box:
  - notify the scrollbox of changes to its size (scrollbox then updates the offset of the pane, and the position of the thumb)

- Grid_pane (or, for the time being, List_pane), as a specialization of Scrollable_pane, adding the abilities:
  - Informing the container (the scrollbox) about the travelling distance for "up" and "down" (separately), which may depend on the height of the first/last item
  
The problem with this is that it requires either unnecessary virtual methods, or two separate specializations of Scrollbox.

An acceptable compromise would be to combine delegated navigation with default navigation, by giving precedence to the former.

To avoid requiring forking Scrollbox, delegated navigation could be introduced in Scrollable_pane as inline no-ops (even constexpr); the concrete type of the content pane could then be injected into Scrollbox as a template parameter.

This appears (and is) somewhat complicated, but does offer the advantage of greater flexibility, and thus less hassle when implementing things like grids and even treeviews later on.

---------

The Scrollbar class is becoming a problem, as one other class to redesign. I believe it is a good strategy to redefine its responsibility now: it should no longer try to keep track of a position by itself (as it does now, in the form of a fraction), but leave that to its client, which will in turn update the position and size of the thumb.

--------------

Idea: implement notification (such as "Position_change" in Scrollbar) as nil-able aspects ?


2016-04-27
==========

There is a need to define an interface between the Scrollbox component and its content.

The current state is working, but treats the content pane as a simple, uniform surface, which can be moved by arbitrary amounts of pixels. This does not take into account the size of items contained in the pane.

What is to be done?

- Implement a completely separate component "Listbox" ? 
  - Or is that name reserved for a component that contains items that are not themselves widgets (indeed typically simple text strings) ?
  
"Listbox":

- A scrollbox combined with a list of widgets

- Those widgets will be arranged into a vertical "stack", the width of each widget set to the "inner width" of the listbox, and its height to the minimum of the highest item (which will get stored to the main aspect and used to configure the scrollbox as well). This will be done as part of the layouting process.
  -> This implies that adding/removing widgets will only be possible if layouting is enabled at least for the Scrollbox component.
  
- Regardless of possible differences in height, each "item widget" counts as a single step navigation-wise

- Layouting of the scrollbox will be based on a configurable number (on the layouter aspect) of "maximum visible items"
  -> NO, that is not possible -> it might be done for get_minimal_size() though

- It is probably best to put the items into a sub-container with "stack" layouting (which may need to be slightly adapted)

2016-04-07
==========

- After implementing Glyph_button, I found that there is currently no way to enforce an aspect ratio, which would make glyph buttons look much nicer.
  -> Future extension ? More sophisticated layouting ?
  
2016-04-05
==========

Working on a text input dialog. Not intended to be reusable yet, except by copy-pasting it into Locsim Instructor.

Several things need work:

- Using MaterialIcons is problematic because apparently the boxing isn't quite correct
  -> THIS APPEARS TO BE WRONG. I've just downloaded and installed a tool called Type Light, and it shows that the MaterialDesign icons are perfectly centered
  -> However, glyphs appear to be systematically shifted by one pixel to the left and the top - but this is actually wrong: the top and left padding is one pixel, where the bottom and right is two pixels - meaning it's impossible at that size to get perfect centering because of grid-fitting.

2016-03-27
==========

- Silly problem: when scrolling with the wheel and the mouse cursor passes onto another child widget without being moved, the highlight does not change until the user actually moves the mouse

2016-03-23
==========

Right now, setting the value of a widget (e.g. "text" on a Textbox) requires a a check (or possibly more later on) to find out whether or not the rasterized font is available.

What is missing at this time is an awareness about whether or not a widget is "live"; or, if maintaining that state turns out to be avoidable, a distinction between *initializing* values versus *updating* them (possibly via overloaded versions of the property setters ?).

I do think, however, that a one-time initialization method is required, to be called after de-serializing or programmatically definining the UI.
If layouting is enabled, the corresponding layout() on the root widget would need to occur immediately before that.

From the above follows a rule:

THE LAYOUT OF A WIDGET *SHOULD NOT* DEPEND ON ITS VALUE (OR SET OF VALUES).

The reason is that this would make it harder to predict the visual appearance of a UI right when it first appears, which is already quite a bit uncertain as soon as font sizes etc. are no longer fixed.

==> TODO: implement:

  1) a virtual method initialize()
  2) property setters that do not trigger invalidation or layout recalculation
      - try to find names that express the difference, rather than employing overloads or prefixed versions of existing names
  
2016-03-14
==========

How to implement trigger_redraw() in a way that will get optimal run-time behaviour on any possible combination of backends ? Brainstorming.

- For a game-type application where frames are redrawn at a high rate independently of GUI interactions, trigger_redraw() could be a no-op
  - Caveat: this does not apply, however, if the GUI is rendered to a dedicated off-screen buffer.
  
- When the only reason to redraw the GUI is in response to user interaction, the best approach is to collect and delay redraw operations until the input event that triggered the need to redraw something has been fully handled.

  - Caveat: the fact that an input event has been fully handled does not necessarily mean that the (partial or full) redraw can or should be done right away. It is conceivable (though probably not very common) that it could happen in another thread. This would make sense if, for example, the GUI is rather complex and rendering it in the same thread as the "event pump" could reduce responsiveness.
  
- For old-style hardware with only 2D acceleration, or when rendering directly to video memory without the help of a GPU (e.g. Linux framebuffer), it can make sense to execute redraws immediately.

- Under the same circumstances, certain operations such as scrolling can be optimized so that only some parts of the affected area need to be redrawn.

- Modern hardware can also help reduce the need to redraw, though in different ways (off-screen buffers / images in video memory ("textures")).

- In rare cases of very low-performance hardware, it could make sense to make redraws interruptible: if filling in an area uncovered by scrolling takes too long, and new input events are already pending, the user experience may be better served by "pushing back" the redraws.

---------------

The above requirements are pretty diverse. It seems important to me that a programmer need not concern himself with all these details, not even when adapting the library for new backends. It is therefore imperative not only to provide abstractions for drawing operations, but also to come up with an impeccable architecture to support said abstractions.

### "Scrolling"

I used the term "scrolling" above, but that is actually a user-space term. "Blitting" was the technical term used for fast transfers of image portions to and from a framebuffer, which is what made scrolling (at reasonable speeds) possible back then. And though such technology is unlikely to come back on PCs, it could be useful on low-power embedded devices.

Contemporary graphical hardware, which is typically 3D-capable, does not need to resort to optimizing image transfers: it has enough memory to store images "whole", and is able to render a whole lot of them at great speed, and at any position.

Supporting these fundamentally different types of hardware requires a higher-level abstraction, so I'm toying with the idea of "virtual spaces".

For example, a listbox could be implemented as a frame that displays a range of "stripes". These stripes would be of fixed height and identified by numbers that would start at 0; however stripes could be added both at the end and before the current first stripe (and possibly inserted anywhere in-between too).

With 2D acceleration (or none), e.g. scrolling down would then translate to "blitting" the bulk of the visibles stripes upward, then triggering a redraw (delayed or not) for the newly uncovered stripe.

With 3D acceleration, the implementation would allocate textures - individually for each strip or, more realistically, of greater size so that each texture could hold several stripes -, and draw the stripes from there.

Caveat: it should be noted that using textures to store the rendered content of list box items would not normally be a worthwhile optimization - though it could be if the rendering is non-trivial. The idea of virtual spaces (1-dimensional in this example) remains valid.

### What's needed

- A draw() method on every widget. Takes into account all state and renders the widget "from scratch".

- A method invalidate() that expresses the need to redraw the widget (by executing draw() as soon as possible)

  - In a game-like rendering loop (without off-screen buffer), invalidate() can be reduced to setting a "dirty" flag.
  
  - The most common case is probably to just delegate to a callback, which is tasked with updating the UI (possibly redrawing layers both
    "below" and "above" the UI)

- All GUI renderer implementations must provide a pair of methods to prepare for and cleaning up after rendering
  
  - This does NOT mean setting the current OpenGL context, which is a platform-specific operation. This must be done by the code, directly called or indirectly triggered by invalidate(), that does the actual rendering.









































